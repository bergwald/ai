{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Overview of Nearest Neighbors Search\n",
    "\n",
    "Nearest neighbors search is an important problem in computer science, with applications ranging from recommender systems to DNA sequencing and cluster analysis. This notebook presents an overview of the problem and of some of the methods developed to solve it.\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. Background\n",
    "    1. Boolean Logic and its Limits\n",
    "    2. k-NN: Formal Definition\n",
    "    3. Representations and Metrics\n",
    "        1. Sequences\n",
    "        2. Vectors\n",
    "2. Data Structures and Algorithms\n",
    "3. NNS in practice: Wikipedia Search\n",
    "    1. Data Import\n",
    "    2. n-grams and Vectorization\n",
    "    3. Brute Force Approach\n",
    "    4. HNSW Approach\n",
    "    5. Addendum: Ranking\n",
    "4. References\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nmslib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from collections import Counter\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from scipy.spatial import distance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "set_matplotlib_formats(\"retina\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Database systems design revolves around **sets** and **relations**. Queries are thus often declared as Boolean expressions, i.e. statements that can be evaluated as true or false. Data exists or doesn't exist, and is related to another piece of data or isn't. The limit of this approach is that only information with low entropy can be reasonably modelled in this logic.\n",
    "\n",
    "An extension to this logic is to specify the spatial structure of a set by defining a **metric**, also known as a **distance function**. By doing so, the elements of the set can be compared and related based on their relative distance. The nearest neighbors search problem arises from this extension. Essentially, it consists in finding the elements in the set that are nearest to a given element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Logic and its Limits\n",
    "\n",
    "Boolean logic, more commonly known as **propositional logic**, is a binary (two-valued; TRUE/FALSE) system of logic. The three basic Boolean operations are conjunction (`AND`, $\\land$), disjunction (`OR`, $\\lor$), and negation (`NOT`, $\\lnot$).\n",
    "\n",
    "*Reminder*: The order of operations, from highest to lowest priority is `NOT`, then `AND`, then `OR`.\n",
    "\n",
    "All other operations can be built by composing these three basic operations. **Logical equality** ($\\leftrightarrow$ or $=$), for example, can be expressed in the forms:\n",
    "\n",
    "$(x = y) = (x \\land y) \\lor (\\lnot x \\land \\lnot y)$\n",
    "\n",
    "`x AND y OR NOT x and NOT y`.\n",
    "\n",
    "The digital logic gate corresponding to logical equality is known as **XNOR**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnor = lambda a, b: (a and b) or (not a and not b)\n",
    "xnor(True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnor(False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, the symbol for the equality operation is `==`. The operation is defined for all types, but for some types compares the identity of the variables rather than their value ([Python docs: Value comparisons](https://docs.python.org/3/reference/expressions.html#value-comparisons))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logical equality in Python\n",
    "True == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strings (`str`) are compared based on their value, defined by the lexicographic order their Unicode code points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"house\" == \"mouse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, the representation of strings depends on its character set. For example, if all characters in a string are in ASCII range, the string is encoded using [latin-1][latin-1] (1 byte per character).\n",
    "\n",
    "For more details, see the blog post [How Python saves memory when storing strings][str mem], the [PEP 393 – Flexible String Representation][PEP 393] specification, and the [unicodeobject.c][unicodeobject.c] file in cpython (particularly lines 1189-1213).\n",
    "\n",
    "[latin-1]: https://en.wikipedia.org/wiki/ISO/IEC_8859-1\n",
    "[str mem]:https://rushter.com/blog/python-strings-and-memory/\n",
    "[PEP 393]: https://peps.python.org/pep-0393/\n",
    "[unicodeobject.c]: https://github.com/python/cpython/blob/main/Objects/unicodeobject.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01101000 01101111 01110101 01110011 01100101'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary representation of a string of latin-1 characters\n",
    "binary = lambda s: \" \".join(map(\"{:08b}\".format, bytearray(s, \"latin-1\")))\n",
    "binary(\"house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01101101 01101111 01110101 01110011 01100101'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary(\"mouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the `==` operation is executed on two strings, the [`unicode_compare_eq`][unicode_compare_eq] function is called. If the strings do not have the same number of characters or are not of the same kind, `unicode_compare_eq` returns `0` (False). Else, the blocks of memory containing the data of both strings are compared using the `memcmp` function, which is a *builtin* in GCC and Clang. `memcmp` compares these blocks and returns `0` if they are equal. Thus, if `memcmp` returns `0`, `unicode_compare_eq` returns `1` (True).\n",
    "\n",
    "[unicode_compare_eq]: https://github.com/python/cpython/blob/main/Objects/unicodeobject.c#L10420-L10439\n",
    "\n",
    "On x86, older compilers such as GCC 4.5.3 translate the `memcmp` function into the `repz cmpsb` assembly instruction. `cmpsb` compares a byte at one address with a byte at another address by subtracting them. `repz` repeats the `cmpsb` operation until the zero flag (ZF) is zero (i.e. the result of the subtraction is not zero) or the count register (CX) is exhausted.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"xnor.svg\" style=\"width:calc(1em * 25); margin:20px;\" />\n",
    "    <div style=\"font-size: 0.85em;\">1-bit half-subtract (A - B) circuit with a zero flag</div>\n",
    "</div>\n",
    "\n",
    "More recent versions of GCC have optimized implementations of `memcmp` that leverage SIMD and AVX instructions.\n",
    "\n",
    "As the foundation of digital electronics, Boolean logic is widely used in higher level abstractions such as software. In SQL, for example, predicates are evaluated to binary truth values, i.e. true/false (sometimes complemented by a third value: unknown). Given a `books` table with `author` and `title` columns, the query\n",
    "\n",
    "```SQL\n",
    "SELECT title\n",
    "FROM books\n",
    "WHERE author=\"Tennessee Williams\";\n",
    "```\n",
    "\n",
    "returns the title of all books whose author is Tennessee Williams. For each relation (row) in the table, the query engine tests whether the predicate `author=\"Tennessee Williams\"` is true or false. Such use of Boolean and predicate logic is common for several reasons:\n",
    "\n",
    "- Ubiquity: Boolean operations are defined on many data types (strings, integers, etc.)\n",
    "- Performance: Boolean expressions can be efficiently evaluated\n",
    "- Simplicity and adoption: Boolean logic is straightforward and widely used\n",
    "- Theory: logic and set theory are part of the foundations of mathematics\n",
    "\n",
    "However, the complexity that can be expressed with a few Boolean operators is limited. Yet, in the real world, data is often ambiguous, inconsistent or incomplete. Applications which need to deal with uncertainty and inaccuracy include, for example:\n",
    "\n",
    "- Genomic analysis: compare genomic sequences\n",
    "- Web search: rank documents on the WWW based on their relevance to a text query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal Definition\n",
    "\n",
    "The ***k*-nearest neighbor (*k*-NN) search problem** can be defined as follows: given a set of points $X$ in a space $S$ and a query point $\\mathbf{q} \\in S$, find the *k* nearest points in $X$ to $q$.\n",
    "\n",
    "$S$ is usually a metric space, i.e. a set with a metric. Sometimes, however, a similarity measure is used instead of the metric.\n",
    "\n",
    "Imagine a database of book titles. The book titles are strings, or in other words, sequences of symbols taken from an alphabet. A title like \"Les Misérables\" is an element in the set of all strings $\\Sigma^*$ over the alphabet $\\Sigma$. If we define a string metric $d$, then $S = (d, \\Sigma^*)$ and $X$ is the set of book titles in the database.\n",
    "\n",
    "As with the book database, a metric can sometimes be defined directly on the data. Types of data for which metrics are readily available include strings, real numbers, and georgraphic coordinates. In other cases, data is transformed into a suitable metric space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representations and Metrics\n",
    "\n",
    "#### Sequences\n",
    "\n",
    "Two common metrics for sequences such as strings are the Hamming distance and the Levenshtein distance.\n",
    "\n",
    "The ***Hamming distance*** is a metric on the set of strings with length $n$. It is the number of positions between two strings at which their corresponding symbols are different.\n",
    "\n",
    "For example, the Hamming distance between \"<span style=\"color:cyan\">h</span>ouse\" and \"<span style=\"color:red\">m</span>ouse\" is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hamming_distance(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    return sum(xi != yi for xi, yi in zip(x, y))\n",
    "\n",
    "hamming_distance(\"house\", \"mouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***Levenshtein distance*** is a metric on the set of all strings. It is the minimum number of single-character edits required to change one string into the other. Unlike the Hamming distance, it is therefore defined for strings of any length and is able to take account of insertions and deletions.\n",
    "\n",
    "For example, the Levenshtein distance between \"kitten\" and \"sitting\" is 3. The <span style=\"color:cyan\">k</span> and <span style=\"color:cyan\">e</span> need to be substituted by an <span style=\"color:red\">s</span> and <span style=\"color:red\">i</span>, and a <span style=\"color:red\">g</span> needs to be inserted at the end:\n",
    "\n",
    "<span style=\"color:cyan\">k</span>itt<span style=\"color:cyan\">e</span>n<span style=\"color:cyan\">_</span><br><span style=\"color:red\">s</span>itt<span style=\"color:red\">i</span>n<span style=\"color:red\">g</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequences are defined by the order of their elements. However, for data such as natural language, units such as words matter more than the index of characters in a sequence of text.\n",
    "\n",
    "One representation for text is the ***bag-of-words model***. It represents text as the bag ([multiset](https://en.wikipedia.org/wiki/Multiset)) of its words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 2,\n",
       "         'author': 1,\n",
       "         'of': 1,\n",
       "         'book': 1,\n",
       "         'is': 1,\n",
       "         'Tennessee': 1,\n",
       "         'Williams': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoW = lambda s: Counter(s.split())\n",
    "BoW(\"the author of the book is Tennessee Williams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words model is a natural vector representation: each word is a dimension. The distance betweeen two elements in the set can be thus defined by a metric such as the cosine distance.\n",
    "\n",
    "A drawback of the bag-of-words model is that it represents only the frequency of words and does not capture their order in the sequence.\n",
    "\n",
    "An alternative representation for sequences such as text is the ***n-gram*** model. An n-gram is a contiguous sub-sequence of *n* items from a given sequence. The unit of an n-gram depends on the elements of the given sequence. For text, it can be words or characters, while for a DNA sequence it might be base pairs. For example, the string \"language\" can be split into 5 character 4-grams:\n",
    "\n",
    "lang, angu, ngua, guag, uage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John likes', 'likes to', 'to watch', 'watch movies']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_ngrams(s: str, n=2):\n",
    "    \"\"\"Splits a string into n-grams of n words\"\"\"\n",
    "    words = s.split()\n",
    "    return [\" \".join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "\n",
    "word_ngrams(\"John likes to watch movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['App', 'ppl', 'ple', 'le ', 'e I', ' In', 'Inc', 'nc.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def char_ngrams(s: str, n=3):\n",
    "    \"\"\"Splits a string into n-grams of n characters\"\"\"\n",
    "    return [s[i:i+n] for i in range(len(s)-n+1)]\n",
    "\n",
    "char_ngrams(\"Apple Inc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between the sets of n-grams of two sequences can be measured using a metric such as the Jaccard distance. The Jaccard distance is a metric on the set of all finite sets. For two sets $A$ and $B$, the Jaccard similarity coefficient (also known as the Jaccard index) is given by the size of their intersection divided by the size of their union:\n",
    "\n",
    "$$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "The Jaccard distance is then obtained by substracting the similarity coefficient from $1$:\n",
    "\n",
    "$$d_{Jaccard}(A, B) = 1 - J(A, B)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard(a: set, b: set):\n",
    "    \"\"\"Jaccard distance between two sets a and b\"\"\"\n",
    "    coeff = len(a & b) / len(a | b)\n",
    "    return 1 - coeff\n",
    "\n",
    "# character 3-grams of two DNA sequences\n",
    "a = (\"ATC\", \"TCG\", \"CGA\", \"GAT\")\n",
    "b = (\"CGA\", \"GAT\", \"ATT\", \"TTG\", \"TGA\")\n",
    "jaccard(set(a), set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard(set(a), set(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, a set of n-gram sequences can be vectorized so that the distance between two sequences is measurable using a metric such as the cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATT</th>\n",
       "      <th>TGA</th>\n",
       "      <th>ATC</th>\n",
       "      <th>CGA</th>\n",
       "      <th>TCG</th>\n",
       "      <th>TTG</th>\n",
       "      <th>GAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ATCGAT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGATTGA</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ATT  TGA  ATC  CGA  TCG  TTG  GAT\n",
       "ATCGAT     0    0    1    1    1    0    1\n",
       "CGATTGA    1    1    0    1    0    1    1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set of n-gram sequences\n",
    "s = set([a, b])\n",
    "# Find the set of all n-grams\n",
    "dims = set.union(*[set(element) for element in s])\n",
    "# Create a feature matrix\n",
    "matrix = np.zeros((len(s), len(dims)))\n",
    "# Populate the matrix\n",
    "for e_idx, element in enumerate(s):\n",
    "    features = Counter(element)\n",
    "    for d_idx, dim in enumerate(dims):\n",
    "        matrix[e_idx][d_idx] = features.get(dim, 0)\n",
    "\n",
    "df = pd.DataFrame(matrix, index=[\"ATCGAT\", \"CGATTGA\"], columns=dims, dtype=int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5527864045000421"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(df.loc[\"ATCGAT\"], df.loc[\"CGATTGA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Word and sentence embeddings*** are two other representations of text. The idea is to represent words or sentences as low-dimensional vectors using a language model such as a neural network. The language model is typically designed to encode the meaning of the word or sentence. Words or sentences with similar meaning or semantic content will thus be close (e.g. small cosine distance) in the vector space.\n",
    "\n",
    "#### Vectors\n",
    "\n",
    "An important subset of metric spaces are normed vector spaces. A vector space is a set of vectors in which two operations are defined: addition and scalar multiplication. Normed vector spaces are vector spaces with a specified norm, i.e. a function which determines the length of all vectors in the vector space. A norm induces a metric, so every normed vector space is a metric space.\n",
    "\n",
    "The $p$-norm or $L^p$ norm of a vector $x$ is defined by\n",
    "\n",
    "$${\\lVert x \\rVert}_p = \\left(\\sum_{i=1}^n |x_i|^p\\right)^{\\frac{1}{p}}$$\n",
    "\n",
    "for a real number $p ≥ 1$. For a given $p$-norm, the corresponding distance (also known as Minkowski distance) between two points $x$ and $y$ is:\n",
    "\n",
    "$$d(x,y) = {\\lVert x-y \\rVert}_p$$\n",
    "\n",
    "Based on $p$, we can distinguish between three metrics:\n",
    "- $p=1$:    ***Manhattan distance*** / ***taxicab metric***\n",
    "\n",
    "$$d_{Manhattan}(x, y) = \\sum_{i=1}^n|x_i-y_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def manhattan(x: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"Manhattan distance between two points x and y\"\"\"\n",
    "    return np.sum(np.abs(x - y))\n",
    "\n",
    "x = np.array([0, 0])\n",
    "y = np.array([2, 1])\n",
    "manhattan(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Manhattan distance can be used when the dimensions of the vector space are not comparable. It is faster to compute than the Euclidean distance and may therefore be preferred in very high dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p=2$: ***Euclidean distance***\n",
    "\n",
    "$$d_{Euclidean}(x, y) = \\left(\\sum_{i=1}^n (x_i-y_i)^2\\right)^{\\frac{1}{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.23606797749979"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def euclidean(x: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"Euclidean distance between two points x and y\"\"\"\n",
    "    return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "euclidean(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, metrics are used only for relative distance comparisons. In such cases, computing the absolute distance is not necessary. For the Euclidean distance, we can therefore omit the square root operation. \n",
    "\n",
    "The Euclidean distance is among the most widely used metrics. However, as the difference between the two points is squared, an outlier coordinate can skew the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p=\\infty$: ***Chebychev distance***\n",
    "\n",
    "The $\\infty$-norm is the limit of the $p$-norm for $p \\to \\infty$:\n",
    "\n",
    "$$\\|x\\|_\\infty = \\lim_{p\\to\\infty} \\|x\\|_p = \\sup_i |x_i|$$\n",
    "\n",
    "In a finite-dimensional vector space, the corresponding distance function is the maximum of the absolute difference between $x$ and $y$:\n",
    "\n",
    "$$d_{Chebychev}(x, y) = \\max_i|x_i -y_i|$$\n",
    "\n",
    "See proof [here](https://proofwiki.org/wiki/Chebyshev_Distance_is_Limit_of_P-Product_Metric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chebyshev(x: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"Chebyshev distance between two points x and y\"\"\"\n",
    "    return np.max(np.abs(x - y))\n",
    "\n",
    "chebyshev(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chebyshev distance is useful when we are interested in the largest difference along any single dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Inner product spaces*** are normed vector spaces with an inner product. An ***inner product*** $\\langle \\cdot, \\cdot \\rangle$ is an operation on two vectors that satisifes the *bilinearity*, *symmetry*, and *positive-definiteness* properties. For example, the real vector space $\\mathbb{R}^n$ with $L^2$ norm, also known as Euclidean space, has an inner product that is known as the dot product. For two vectors $x$ and $y$:\n",
    "\n",
    "$$\\langle x, y \\rangle = \\sum_i^n x_i y_i = x \\cdot y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([-1, 3])\n",
    "y = np.array([.5, 2])\n",
    "\n",
    "np.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product indicates the angle and magnitude of two vectors. Two vectors with a small angle can thus have a smaller dot product than two vectors with a larger angle but higher magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x, x) > np.dot(x**2, y**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By dividing the vectors by their norms, i.e. converting them into unit vectors, the effect of magnitude can be eliminated. The result of this operation is known as the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x / np.linalg.norm(x)\n",
    "y = x / np.linalg.norm(x)\n",
    "np.dot(x, x) > np.dot(x**2, y**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***Cosine similarity*** is a measure of the similarity of two vectors $x$ and $y$, and is defined as the cosine of the angle $\\theta$ between them:\n",
    "\n",
    "$$S_C(x, y) := \\cos(\\theta) = \\frac{x \\cdot y}{\\|x\\| \\|y\\|} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8436614877321075"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine(x: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"Cosine similarity between two points x and y\"\"\"\n",
    "    assert np.any(x) == True and np.any(y) == True\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "x = np.array([-1, 3])\n",
    "y = np.array([.5, 2])\n",
    "cosine(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product and cosine similarity are not metrics, but are often used in information retrieval when text is represented as term/n-gram frequency vectors or as word/sentence embeddings.\n",
    "\n",
    "Proportional vectors have a cosine similarity of $1$, orthogonal vectors have a similarity of $0$, and opposite vectors have a similarity of $-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthogonal = np.array([3, 1])\n",
    "cosine(x, orthogonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9999999999999998"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opposite = np.array([1, -3])\n",
    "cosine(x, opposite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures and Algorithms\n",
    "\n",
    "A brute force solution to the *k*-NN search problem is to compute the distance between the query point and each point in dataset $X$, and loop *k* times through the results to return the *k* points with the smallest distance. The downside of this approach is that it has a high computational complexity. Let's imagine, for example, that $S$ is a vector space $\\mathbb{R}^d$ with $d$ dimensions and that the Euclidean distance is used as the metric. Each distance computation has a time complexity of $O(d)$. Computing the distances for $n$ points in $X$ therefore requires $O(nd)$ runtime. At last, selecting the *k* smallest distances requires $O(nk)$ runtime. The time complexity of this brute force algorithm is therefore $O(nd + nk)$. If we have a high number of dimensions, a large number of samples, or need to evaluate multiple queries, this approach is unworkable.\n",
    "\n",
    "A more refined approach is to build an index of the spatial structure of the dataset $X$. \n",
    "\n",
    "General metric spaces:\n",
    "- **vp-tree** (vantage point tree): a tree that partitions space into smaller and smaller circles. The root of the tree is an element of $S$ that serves as the \"vantage point\". Elements that are within a certain distance (radius) of this point are on one side of the node, while elements outside of the radius are on the other side of the node. Partitioning space with this method yields a tree of increasingly smaller circles.\n",
    "- **BK-tree** (Burkhard tree): a tree designed specifically for discrete metrics (e.g. Levenshtein distance; Manhattan and Chebyshev distance on a space of natural numbers). Each node represents an element of $S$, and links between nodes indicate the distance between them.\n",
    "\n",
    "Euclidean spaces:\n",
    "- ***k*-d tree** (*k*-dimensional tree): a type of binary search tree. Every node in the tree is a point in *k*-dimensional space. The two childs of every node are points on the right and left side of a hyperplane along a certain dimension.\n",
    "- **r-tree**: a tree where groups of nearby points are represented by their minimum bounding rectangle (hence the name r-tree). Each node represents a rectangle and links to its sub-rectangles. The r-tree can therefore be thought of a tree of increasingly smaller rectangles.\n",
    "\n",
    "Levenshtein spaces:\n",
    "- **trie** (also know as **prefix tree**): a search tree where each node is a partial or complete sequence and links between nodes represent individual elements.\n",
    "\n",
    "Depth-first search (DFS) of these data structures yields exact results. DFS of exact space indexes works well on low dimensional spaces, but is expensive on high-dimensional ones. For high dimensions, this method is not much more efficient than the brute force approach.\n",
    "\n",
    "<!-- See \"A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces\" -->\n",
    "\n",
    "An alternative approach for high-dimensional spaces is to relax the constraints around the problem. Instead of searching for the exact nearest neighbors, only the approximate nearest neighbors are searched for. The idea behind **approximate nearest neighbors (ANN) search** is to leverage quantization, i.e. reduce the cardinality of the representation, to improve search efficiency. Essentially, the dataset is compressed with loss before being indexed. ANN search therefore makes a tradeoff between search quality and efficiency.\n",
    "\n",
    "One technique to perform quantization is **locality sensitive hashing** (LSH). A locality senstive hash function maps points from $S$ into a lower dimensional representation that can be efficiently indexed and queried.\n",
    "\n",
    "In Euclidean space, another technique is **product quantization**. It partitions a high dimensional space into a Cartesian product of low dimensional subspaces and quantizes each subspace separately (Jégou, Douze, & Schmid, 2011). The distance between a query vector and a PQ-encoded vector can be estimated with low runtime cost (this is known as asymmetric distance computation). Based on product quantization, an **inverted index** of the encoded dataset can be built to efficiently search for nearest neighbors. The inverted index proposed by Jégou, Douze, & Schmid (2011) is similar to the inverted file system of Sivic and Zisserman (2003).\n",
    "\n",
    "The **Hierarchical Navigable Small World** graph (HNSW) is a proximity graph, i.e. its vertices are linked based on their proximity in space (Malkov & Yashunin, 2016). HNSW is inspired by two data structures: *skip lists* and *navigable small world* graphs (NSW). Skip lists consist of layered linked list that allow for fast search. NSW models are graphs with (poly/)logarithmic search complexity that use a greedy routing algorithm. HNSW consists of layered NSW graphs, with a hierachical multi-structure similar to the skip list.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"hnsw.png\" style=\"width:calc(1em * 20); margin:20px; background-color: white;\" alt=\"Hierarchical Navigable Small World graph\"/>\n",
    "    <div style=\"font-size: 0.85em;\">Hierarchical Navigable Small World graph (Malkov & Yashunin, 2016)</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNS in Practice: Wikipedia Search\n",
    "\n",
    "Let's look at how NNS search can be implemented in practice. We'll implement a Wikipedia search engine. For simplicity, we'll index only the names of computer scientists with an article on the English Wikipedia. The problem to be solved can thus be stated as follows: **given the names of all computer scientists on Wikipedia and a string query, find the *k* names that are nearest to the query**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import\n",
    "\n",
    "The data is imported from Wikidata by querying for items whose `occupation` property (P106) is `computer scientist` (Q82594) or a subclass thereof, and who are part of the English Wikipedia.\n",
    "\n",
    "The SPARQL query is optimized to use the label service. See [Wikidata:SPARQL query service/query optimization](https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/query_optimization#Label_service)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://query.wikidata.org/sparql\"\n",
    "query = \"\"\"\n",
    "SELECT ?itemLabel ?sitelink ?linkcount\n",
    "WHERE {\n",
    "  {\n",
    "    SELECT DISTINCT ?item ?sitelink ?linkcount WHERE {\n",
    "      ?item p:P106 ?statement.\n",
    "      ?statement (ps:P106/(wdt:P279*)) wd:Q82594.\n",
    "      ?sitelink schema:about ?item;\n",
    "      schema:isPartOf <https://en.wikipedia.org/>.\n",
    "      ?item wikibase:sitelinks ?linkcount.\n",
    "    }\n",
    "  }\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {\"format\": \"json\", \"query\": query})\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response is then extracted into a Pandas `DataFrame`. `url` is the URL of the Wikipedia article and `links` is the number of [interwiki links](https://www.wikidata.org/wiki/Help:Sitelinks) to the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Steven Jobs</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Steve_Jobs</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stormy Peters</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Stormy_Peters</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wanda Orlikowski</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Wanda_Orlikowski</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Susan Owicki</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Susan_Owicki</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Colette Rolland</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Colette_Rolland</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5925</th>\n",
       "      <td>Daniel J. Hulme</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Daniel_J._Hulme</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5926</th>\n",
       "      <td>Alexander Reben</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alexander_Reben</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5927</th>\n",
       "      <td>Nils John Nilsson</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Nils_John_Nilsson</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5928</th>\n",
       "      <td>Jean-Christophe Baillie</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jean-Christophe_...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5929</th>\n",
       "      <td>Gary B. Fogel</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Gary_B._Fogel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5930 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name  \\\n",
       "0                 Steven Jobs   \n",
       "1               Stormy Peters   \n",
       "2            Wanda Orlikowski   \n",
       "3                Susan Owicki   \n",
       "4             Colette Rolland   \n",
       "...                       ...   \n",
       "5925          Daniel J. Hulme   \n",
       "5926          Alexander Reben   \n",
       "5927        Nils John Nilsson   \n",
       "5928  Jean-Christophe Baillie   \n",
       "5929            Gary B. Fogel   \n",
       "\n",
       "                                                    url  links  \n",
       "0              https://en.wikipedia.org/wiki/Steve_Jobs    206  \n",
       "1           https://en.wikipedia.org/wiki/Stormy_Peters      6  \n",
       "2        https://en.wikipedia.org/wiki/Wanda_Orlikowski      7  \n",
       "3            https://en.wikipedia.org/wiki/Susan_Owicki      2  \n",
       "4         https://en.wikipedia.org/wiki/Colette_Rolland      6  \n",
       "...                                                 ...    ...  \n",
       "5925      https://en.wikipedia.org/wiki/Daniel_J._Hulme      1  \n",
       "5926      https://en.wikipedia.org/wiki/Alexander_Reben      1  \n",
       "5927    https://en.wikipedia.org/wiki/Nils_John_Nilsson      9  \n",
       "5928  https://en.wikipedia.org/wiki/Jean-Christophe_...      3  \n",
       "5929        https://en.wikipedia.org/wiki/Gary_B._Fogel      1  \n",
       "\n",
       "[5930 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = r.json()\n",
    "data = []\n",
    "for result in raw[\"results\"][\"bindings\"]:\n",
    "    name = result[\"itemLabel\"][\"value\"]\n",
    "    url = result[\"sitelink\"][\"value\"]\n",
    "    links = int(result[\"linkcount\"][\"value\"])\n",
    "    data.append((name, url, links))\n",
    "df = pd.DataFrame(data, columns=[\"name\", \"url\", \"links\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *n*-grams and Vectorization\n",
    "\n",
    "Let's represent the names of the authors as a count matrix of their 3-grams. This count matrix will be the basis of the search index. We'll use the cosine distance to measure the similarity between query vectors and name vectors.\n",
    "\n",
    "***Design note:*** since we are working with strings, we could have chosen a string metric such as the Levenshtein distance and index the space with a trie. However, this would have meant that results are dependent on the order of names/words in the query. For example, the queries \"Alan Turing\" and \"Turing Alan\" may have returned different results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ste', 'tev', 'eve', 'ven', 'en ', 'n J', ' Jo', 'Job', 'obs']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the character 3-grams sequence of Steven Jobs\n",
    "char_ngrams(\"Steven Jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the matrix of 3-gram counts using scikit-learn's `CountVectorizer` together with the `char_grams` function defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5930, 8681)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer=char_ngrams, lowercase=False)\n",
    "count_matrix = vectorizer.fit_transform(list(df[\"name\"]))\n",
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting vector space has 8681 dimensions.\n",
    "\n",
    "Next, we transform the test queries using the learned vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8681)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [\"Jobs\", \"Cobb\", \"Jeff\", \"Turing\"]\n",
    "query_matrix = vectorizer.transform(queries)\n",
    "query_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute Force Approach\n",
    "\n",
    "The simplest approach is to do a brute force search, i.e. calculate the distance between the query and all the points in the dataset, and return the *k* points with the smallest distance.\n",
    "\n",
    "scikit-learn's `NearestNeighbors` is a practical interface to some nearest neighbors search algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Jobs\n",
      "                             Steven Jobs\n",
      "https://en.wikipedia.org/wiki/Steve_Jobs\n",
      "Cosine distance: 0.5286\n",
      "                              Van Jacobson\n",
      "https://en.wikipedia.org/wiki/Van_Jacobson\n",
      "Cosine distance: 0.7764\n",
      "                              Ivar Jacobson\n",
      "https://en.wikipedia.org/wiki/Ivar_Jacobson\n",
      "Cosine distance: 0.7868\n"
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "kNN = NearestNeighbors(n_neighbors=K, algorithm=\"brute\", metric=\"cosine\")\n",
    "kNN.fit(count_matrix)\n",
    "\n",
    "# 3 nearest neighbors of the query `Jobs`\n",
    "print(\"Query: Jobs\")\n",
    "distances, neighbors = kNN.kneighbors(query_matrix[0])\n",
    "for distance, neighbor in zip(distances[0], neighbors[0]):\n",
    "    print(df.iloc[neighbor][:2].to_string(index=False))\n",
    "    print(f\"Cosine distance: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Jobs\n",
      "                             Steven Jobs\n",
      "https://en.wikipedia.org/wiki/Steve_Jobs\n",
      "Cosine distance: 0.5286\n",
      "Query: Cobb\n",
      "                              Chris Cobb\n",
      "https://en.wikipedia.org/wiki/Chris_Cobb\n",
      "Cosine distance: 0.5000\n",
      "Query: Jeff\n",
      "                                       Jeff Moss\n",
      "https://en.wikipedia.org/wiki/Jeff_Moss_(hacker)\n",
      "Cosine distance: 0.4655\n",
      "Query: Turing\n",
      "                              Alan Turing\n",
      "https://en.wikipedia.org/wiki/Alan_Turing\n",
      "Cosine distance: 0.3333\n"
     ]
    }
   ],
   "source": [
    "# Nearest neighbor (k=1) of each query\n",
    "K = 1\n",
    "distances, neighbors = kNN.kneighbors(query_matrix, n_neighbors=K)\n",
    "for query, neighbor, distance in zip(queries, neighbors, distances):\n",
    "    print(f\"Query: {query}\")\n",
    "    print(df.iloc[neighbor.item()][:2].to_string(index=False))\n",
    "    print(f\"Cosine distance: {distance.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.88 ms ± 254 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit kNN.kneighbors(query_matrix, n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HNSW Approach\n",
    "\n",
    "Recent research on ANN algorithms and machine learning embeddings has led to the creation of a number of ANN libraries. Examples of such libraries are *Facebook AI Similarity Search* (FAISS) and *Non-Metric Space Library* (NMSLIB). For an overview and benchmarks of these libraries, see [ANN Benchmarks](https://github.com/erikbern/ann-benchmarks).\n",
    "\n",
    "Unlike other libraries which are limited to dense spaces, NMSLIB can index sparse spaces. We use an HNSW graph and use the cosine similarity as the distance function. The parameters `M` and `efConstruction` determine the quality of the constructed graph and therefore impact the accuracy and recall of search. The [NMSLIB documentation](https://github.com/nmslib/nmslib/blob/master/manual/methods.md) provided a brief overview of these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HNSW construction graph parameters\n",
    "M = 30\n",
    "efConstruction = 200\n",
    "\n",
    "index = nmslib.init(\n",
    "    method=\"hnsw\",\n",
    "    space=\"cosinesimil_sparse\",\n",
    "    data_type=nmslib.DataType.SPARSE_VECTOR\n",
    ")\n",
    "index.addDataPointBatch(count_matrix)\n",
    "index.createIndex({\n",
    "    \"M\": M,\n",
    "    \"efConstruction\": efConstruction,\n",
    "    \"indexThreadQty\": 2, \n",
    "    \"post\": 0 # No post-processing of the graph\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the `efSearch` parameter influences recall and search time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.setQueryTimeParams({\"efSearch\": 150})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Jobs\n",
      "                             Steven Jobs\n",
      "https://en.wikipedia.org/wiki/Steve_Jobs\n",
      "Cosine distance: 0.5286\n",
      "                              Van Jacobson\n",
      "https://en.wikipedia.org/wiki/Van_Jacobson\n",
      "Cosine distance: 0.7764\n",
      "                              Ivar Jacobson\n",
      "https://en.wikipedia.org/wiki/Ivar_Jacobson\n",
      "Cosine distance: 0.7868\n"
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "neighbors, distances = index.knnQueryBatch(query_matrix[0], k=K)[0]\n",
    "\n",
    "print(\"Query: Jobs\")\n",
    "for neighbor, distance in zip(neighbors, distances):\n",
    "    print(df.iloc[neighbor.item()][:2].to_string(index=False))\n",
    "    print(f\"Cosine distance: {distance.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Jobs\n",
      "                             Steven Jobs\n",
      "https://en.wikipedia.org/wiki/Steve_Jobs\n",
      "Cosine distance: 0.5286\n",
      "Query: Cobb\n",
      "                              Chris Cobb\n",
      "https://en.wikipedia.org/wiki/Chris_Cobb\n",
      "Cosine distance: 0.5000\n",
      "Query: Jeff\n",
      "                              Jeff Dean\n",
      "https://en.wikipedia.org/wiki/Jeff_Dean\n",
      "Cosine distance: 0.4655\n",
      "Query: Turing\n",
      "                              Alan Turing\n",
      "https://en.wikipedia.org/wiki/Alan_Turing\n",
      "Cosine distance: 0.3333\n"
     ]
    }
   ],
   "source": [
    "K = 1\n",
    "results = index.knnQueryBatch(query_matrix, k=K)\n",
    "for query, result in zip(queries, results):\n",
    "    nneighbor, distance = result\n",
    "    print(f\"Query: {query}\")\n",
    "    print(df.iloc[nneighbor.item()][:2].to_string(index=False))\n",
    "    print(f\"Cosine distance: {distance.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HNSW graph requires time to be built, but querying it is somewhat faster than doing a brute force search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.87 ms ± 427 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit index.knnQueryBatch(query_matrix, k=K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addendum: Ranking\n",
    "\n",
    "String similarity alone is a poor statistic to rank the retrieved results. Consider the query \"Jeff\". The string \"Jeff Bezos\" is 10 characters long and has therefore a higher cosine distance to the query \"Jeff\" than shorter strings. Yet Jeff Bezos is more likely to be the subject of a search than other computer scientists. Search engines and recommender systems therefore often use a multitude of statistics to rank results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Jeff\n",
      "Jeff Moss       Cosine dist.:0.4655\n",
      "Jeff Dean       Cosine dist.:0.4655\n",
      "Jeff Skoll      Cosine dist.:0.5000\n",
      "Jeff Bezos      Cosine dist.:0.5000\n",
      "Jeff Black      Cosine dist.:0.5000\n",
      "Jeff Raikes     Cosine dist.:0.5286\n",
      "Jeff Pulver     Cosine dist.:0.5286\n",
      "Jeff Atwood     Cosine dist.:0.5286\n",
      "Jeff Minter     Cosine dist.:0.5286\n",
      "Jeff Bonwick    Cosine dist.:0.5528\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "neighbors, distances = index.knnQueryBatch(query_matrix[2], k=K)[0]\n",
    "\n",
    "print(\"Query: Jeff\")\n",
    "for neighbor, distance in zip(neighbors, distances):\n",
    "    name = df.iloc[neighbor.item()][0]\n",
    "    print(f\"{name:<15} Cosine dist.:{distance.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interwiki links can be used as a proxy for the importance/prominence/popularity of a Wikimedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 287,
       "width": 387
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df[\"links\"], bins=100, alpha=0.75, facecolor=\"g\")\n",
    "plt.xlabel(\"Links\")\n",
    "plt.xlim(left=0)\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"\"\"Distribution of Interwiki Links to Articles\n",
    "    of Computer Scientists on the English Wikipedia\"\"\", fontsize=10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5008</th>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bill_Gates</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3618</th>\n",
       "      <td>Jimmy Wales</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jimmy_Wales</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Steven Jobs</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Steve_Jobs</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3617</th>\n",
       "      <td>Alan Turing</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Alan_Turing</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3875</th>\n",
       "      <td>Noam Chomsky</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Noam_Chomsky</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4077</th>\n",
       "      <td>Tim Berners-Lee</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Tim_Berners-Lee</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4776</th>\n",
       "      <td>Linus Torvalds</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Linus_Torvalds</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>Mark Zuckerberg</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mark_Zuckerberg</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Julian Assange</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Julian_Assange</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3303</th>\n",
       "      <td>Edward Snowden</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Edward_Snowden</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name                                            url  links\n",
       "5008       Bill Gates       https://en.wikipedia.org/wiki/Bill_Gates    214\n",
       "3618      Jimmy Wales      https://en.wikipedia.org/wiki/Jimmy_Wales    210\n",
       "0         Steven Jobs       https://en.wikipedia.org/wiki/Steve_Jobs    206\n",
       "3617      Alan Turing      https://en.wikipedia.org/wiki/Alan_Turing    168\n",
       "3875     Noam Chomsky     https://en.wikipedia.org/wiki/Noam_Chomsky    162\n",
       "4077  Tim Berners-Lee  https://en.wikipedia.org/wiki/Tim_Berners-Lee    147\n",
       "4776   Linus Torvalds   https://en.wikipedia.org/wiki/Linus_Torvalds    143\n",
       "2435  Mark Zuckerberg  https://en.wikipedia.org/wiki/Mark_Zuckerberg    141\n",
       "247    Julian Assange   https://en.wikipedia.org/wiki/Julian_Assange    129\n",
       "3303   Edward Snowden   https://en.wikipedia.org/wiki/Edward_Snowden    113"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(\"links\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of fusing the interwiki links and string similarity statistics is to multiply them together. The relevance of an article to a query can thus be defined as: `relevance = cosine similarity * number of interwiki links`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cosine similarity is calculated\n",
    "# by subtracting the cosine distance from one.\n",
    "\n",
    "def rank(neighbors: np.ndarray, cosine_dist: np.ndarray):\n",
    "    \"\"\"Rank and enrich the results of the k-NN query\"\"\"\n",
    "    links = df.iloc[neighbors].links\n",
    "    # Compute the relevance of each neighbor\n",
    "    relevance = (1 - cosine_dist) * links\n",
    "    relevance.rename(\"relevance\", inplace=True)\n",
    "    # Add metadata\n",
    "    results = df.merge(relevance, left_index=True, right_index=True)\n",
    "    # Sort the results by relevance\n",
    "    results = results.sort_values(by=\"relevance\", ascending=False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking the retrieved articles by their relevance score yields the following results: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Jeff\n",
      "               cosine similarity * links = relevance score\n",
      "Jeff Bezos     0.500000 * 97 = 48.5000\n",
      "Jeff Skoll     0.500000 * 14 = 7.0000\n",
      "Jeff Dean      0.534523 * 12 = 6.4143\n",
      "Jeff Atwood    0.471405 * 12 = 5.6569\n",
      "Jeff Minter    0.471405 * 10 = 4.7140\n",
      "Jeff Moss      0.534523 * 7  = 3.7417\n",
      "Jeff Pulver    0.471405 * 5  = 2.3570\n",
      "Jeff Black     0.500000 * 2  = 1.0000\n",
      "Jeff Raikes    0.471405 * 2  = 0.9428\n",
      "Jeff Bonwick   0.447214 * 1  = 0.4472\n"
     ]
    }
   ],
   "source": [
    "print(\"Query: Jeff\")\n",
    "print(\" \"*15 + \"cosine similarity * links = relevance score\")\n",
    "\n",
    "# Query the HNSW index\n",
    "K = 10\n",
    "neighbors, cosine_dists = index.knnQueryBatch(query_matrix[2], k=K)[0]\n",
    "\n",
    "# Rank the results\n",
    "results = rank(neighbors, cosine_dists)\n",
    "\n",
    "results = results.join(pd.DataFrame(cosine_dists, index=neighbors, columns=[\"cosine\"]))\n",
    "for _, result in results.iterrows():\n",
    "    print(f\"{result['name']:<15}{1 - result.cosine:.6f} * {result.links:<2} = {result.relevance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `search` function retrieves and ranks the top *k* nearest neighbors for any given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query: str, k: int):\n",
    "    \"\"\"Search for Computer Scientists on Wikipedia\"\"\"\n",
    "    # Vectorize the query\n",
    "    vectorized_query = vectorizer.transform([query])\n",
    "    # Search the HNSW graph\n",
    "    neighbors, cosine = index.knnQueryBatch(vectorized_query, k=k)[0]\n",
    "    # Rank the results\n",
    "    results = rank(neighbors, cosine)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>links</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3387</th>\n",
       "      <td>Marc Ewing</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Marc_Ewing</td>\n",
       "      <td>7</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4224</th>\n",
       "      <td>Marc Snir</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Marc_Snir</td>\n",
       "      <td>3</td>\n",
       "      <td>1.603568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5350</th>\n",
       "      <td>Marc Levoy</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Marc_Levoy</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4117</th>\n",
       "      <td>Marc Vael</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Marc_Vael</td>\n",
       "      <td>1</td>\n",
       "      <td>0.534523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>Marc Alexa</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Marc_Alexa</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name                                       url  links  relevance\n",
       "3387  Marc Ewing  https://en.wikipedia.org/wiki/Marc_Ewing      7   3.500000\n",
       "4224   Marc Snir   https://en.wikipedia.org/wiki/Marc_Snir      3   1.603568\n",
       "5350  Marc Levoy  https://en.wikipedia.org/wiki/Marc_Levoy      2   1.000000\n",
       "4117   Marc Vael   https://en.wikipedia.org/wiki/Marc_Vael      1   0.534523\n",
       "3731  Marc Alexa  https://en.wikipedia.org/wiki/Marc_Alexa      1   0.500000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: `Marc` query\n",
    "search(\"Marc\", k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real world systems usually retrieve and rank a higher number of results than displayed. For example, the aggregate relevance of the top 5 results for the query \"Marc\" increases with the number of retrieved nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "      <th>links</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Marc Andreessen</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Marc_Andreessen</td>\n",
       "      <td>31</td>\n",
       "      <td>12.159201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>Marcian Hoff</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Marcian_Hoff</td>\n",
       "      <td>22</td>\n",
       "      <td>9.838699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>Solomon Marcus</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Solomon_Marcus</td>\n",
       "      <td>17</td>\n",
       "      <td>6.940221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>Tom DeMarco</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Tom_DeMarco</td>\n",
       "      <td>14</td>\n",
       "      <td>6.599664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1527</th>\n",
       "      <td>Marco Dorigo</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Marco_Dorigo</td>\n",
       "      <td>8</td>\n",
       "      <td>3.577709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name                                            url  links  \\\n",
       "38    Marc Andreessen  https://en.wikipedia.org/wiki/Marc_Andreessen     31   \n",
       "1953     Marcian Hoff     https://en.wikipedia.org/wiki/Marcian_Hoff     22   \n",
       "1899   Solomon Marcus   https://en.wikipedia.org/wiki/Solomon_Marcus     17   \n",
       "1747      Tom DeMarco      https://en.wikipedia.org/wiki/Tom_DeMarco     14   \n",
       "1527     Marco Dorigo     https://en.wikipedia.org/wiki/Marco_Dorigo      8   \n",
       "\n",
       "      relevance  \n",
       "38    12.159201  \n",
       "1953   9.838699  \n",
       "1899   6.940221  \n",
       "1747   6.599664  \n",
       "1527   3.577709  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"Marc\", k=20).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Principles of Mathematical Analysis (Rudin, 1976)\n",
    "- Metric space (Wikipedia) [Link](https://en.wikipedia.org/wiki/Metric_space)\n",
    "- String metric (Wikipedia) [Link](https://en.wikipedia.org/wiki/String_metric)\n",
    "- Introduction to the Theory of Computation (Sipser, 2013)\n",
    "- An Introduction to Information Retrieval (Manning et al., 2009)\n",
    "- Data Structures and Algorithms for Nearest Neighbor Search in General Metric Spaces (P. N. Yianilos, 1993) [PDF](http://algorithmics.lsi.upc.edu/docs/practicas/p311-yianilos.pdf)\n",
    "- Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs (Y. A. Malkov, D. A. Yashunin, 2016) [Arxiv](https://arxiv.org/abs/1603.09320)\n",
    "- Billion-scale similarity search with GPUs (J. Johnson, M. Douze, H. Jégou, 2019) [Arxiv](https://arxiv.org/abs/1702.08734) / [Github: FAISS library](https://github.com/facebookresearch/faiss)\n",
    "- ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms (M. Aumüller, E. Bernhardsson, A. Faithfull, 2019) [Arxiv](https://arxiv.org/abs/1807.05614) / [Github](https://github.com/erikbern/ann-benchmarks)\n",
    "- Engineering Efficient and Effective Non-Metric Space Library (L. Boytsov, B. Naidan, 2013) [PDF](http://boytsov.info/pubs/sisap2013.pdf) / [Github: NMSLIB](https://github.com/nmslib/nmslib)\n",
    "- Product Quantization for Nearest Neighbor Search (H. Jégou, M. Douze, C. Schmid,  2011) [HAL-Inria](https://hal.inria.fr/inria-00514462v2)\n",
    "- Video Google: A Text Retrieval Approach to Object Matching in Videos (J. Sivic, and A. Zisserman, 2003) [PDF](https://www.robots.ox.ac.uk/~vgg/publications/papers/sivic03.pdf)\n",
    "- Database system concepts, 7th edition (A. Silberschatz, H. F. Korth, S. Sudarshan, 2019)\n",
    "- A relational model of data for large shared data banks (E. F. Codd, 1970) [PDF](https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf)\n",
    "- Relational Algebra (Wikipedia) [Link](https://en.wikipedia.org/wiki/Relational_algebra)\n",
    "- Wikidata: SPARQL tutorial (Wikidata) [Link](https://www.wikidata.org/wiki/Wikidata:SPARQL_tutorial)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2bb61274428d8154af2dff0fdb211830dcce3790af96cc102fcdb2021a644ac5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
