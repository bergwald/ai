{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Overview of Nearest Neighbors Search\n",
    "\n",
    "Nearest neighbors search is an important problem in computer science, with applications ranging from recommender systems to DNA sequencing and cluster analysis. This notebook presents an overview of the problem and of some of the methods developed to solve it.\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. [Background](#background)\n",
    "    - [Boolean Logic and its Limits](#boolean-logic-and-its-limits)\n",
    "    - [k-NN: Formal Definition](#formal-definition)\n",
    "    - [Representations and Metrics](#representations-and-metrics)\n",
    "      - [Sequences](#sequences)\n",
    "      - [Vectors](#vectors)\n",
    "2. [Data Structures and Algorithms](#data-structures-and-algorithms)\n",
    "3. [NNS in practice: Wikipedia Search](#nns-in-practice-wikipedia-search)\n",
    "    - [Data Import](#data-import)\n",
    "    - [n-grams and Vectorization](#n-grams-and-vectorization)\n",
    "    - [Brute Force Approach](#brute-force-approach)\n",
    "    - [HNSW Approach](#hnsw-approach)\n",
    "4. [References](#references)\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from collections import Counter\n",
    "from scipy.spatial import distance\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Database systems design revolves around **sets** and **relations**. Queries are thus often declared as Boolean expressions, i.e. statements that can be evaluated as true or false. Data exists or doesn't exist, and is related to another piece of data or isn't. The limit of this approach is that only information with low entropy can be reasonably modelled in this logic.\n",
    "\n",
    "An extension to this logic is to specify the spatial structure of a set by defining a **metric**, also known as a **distance function**. By doing so, the elements of the set can be compared and related based on their relative distance. The nearest neighbors search problem arises from this extension. Essentially, it consists in finding the elements in the set that are nearest to a given element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boolean Logic and its Limits\n",
    "\n",
    "Boolean logic, more commonly known as **propositional logic**, is a binary (two-valued; TRUE/FALSE) system of logic. The three basic Boolean operations are conjunction (`AND`, $\\land$), disjunction (`OR`, $\\lor$), and negation (`NOT`, $\\lnot$).\n",
    "\n",
    "*Reminder*: The order of operations, from highest to lowest priority is `NOT`, then `AND`, then `OR`.\n",
    "\n",
    "All other operations can be built by composing these three basic operations. **Logical equality** ($\\leftrightarrow$ or $=$), for example, can be expressed in the forms:\n",
    "\n",
    "$(x = y) = (x \\land y) \\lor (\\lnot x \\land \\lnot y)$\n",
    "\n",
    "`x AND y OR NOT x and NOT y`.\n",
    "\n",
    "The digital logic gate corresponding to logical equality is known as **XNOR**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnor = lambda a, b: (a and b) or (not a and not b)\n",
    "xnor(True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xnor(False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, the symbol for the equality operation is `==`. The operation is defined for all types, but for some types compares the identity of the variables rather than their value ([Python docs: Value comparisons](https://docs.python.org/3/reference/expressions.html#value-comparisons))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logical equality in Python\n",
    "True == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strings (`str`) are compared based on their value, defined by the lexicographic order their Unicode code points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"house\" == \"mouse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, the representation of strings depends on its character set. For example, if all characters in a string are in ASCII range, the string is encoded using [latin-1][latin-1] (1 byte per character).\n",
    "\n",
    "For more details, see the blog post [How Python saves memory when storing strings][str mem], the [PEP 393 â€“ Flexible String Representation][PEP 393] specification, and the [unicodeobject.c][unicodeobject.c] file in cpython (particularly lines 1189-1213).\n",
    "\n",
    "[latin-1]: https://en.wikipedia.org/wiki/ISO/IEC_8859-1\n",
    "[str mem]:https://rushter.com/blog/python-strings-and-memory/\n",
    "[PEP 393]: https://peps.python.org/pep-0393/\n",
    "[unicodeobject.c]: https://github.com/python/cpython/blob/main/Objects/unicodeobject.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01101000 01101111 01110101 01110011 01100101'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binary representation of a string of latin-1 characters\n",
    "binary = lambda s: \" \".join(map(\"{:08b}\".format, bytearray(s, \"latin-1\")))\n",
    "binary(\"house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01101101 01101111 01110101 01110011 01100101'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary(\"mouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the `==` operation is executed on two strings, the [`unicode_compare_eq`][unicode_compare_eq] function is called. If the strings do not have the same number of characters or are not of the same kind, `unicode_compare_eq` returns `0` (False). Else, the blocks of memory containing the data of both strings are compared using the `memcmp` function, which is a *builtin* in GCC and Clang. `memcmp` compares these blocks and returns `0` if they are equal. Thus, if `memcmp` returns `0`, `unicode_compare_eq` returns `1` (True).\n",
    "\n",
    "[unicode_compare_eq]: https://github.com/python/cpython/blob/main/Objects/unicodeobject.c#L10420-L10439\n",
    "\n",
    "On x86, older compilers such as GCC 4.5.3 translate the `memcmp` function into the `repz cmpsb` assembly instruction. `cmpsb` compares a byte at one address with a byte at another address by subtracting them. `repz` repeats the `cmpsb` operation until the zero flag (ZF) is zero (i.e. the result of the subtraction is not zero) or the count register (CX) is exhausted.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"xnor.svg\" style=\"width:calc(1em * 25); margin:20px;\" />\n",
    "    <div style=\"font-size: 0.85em;\">1-bit half-subtract (A - B) circuit with a zero flag</div>\n",
    "</div>\n",
    "\n",
    "More recent versions of GCC have optimized implementations of `memcmp` that leverage SIMD and AVX instructions.\n",
    "\n",
    "As the foundation of digital electronics, Boolean logic is widely used in higher level abstractions such as software. In SQL, for example, predicates are evaluated to binary truth values, i.e. true/false (sometimes complemented by a third value: unknown). Given a `books` table with `author` and `title` columns, the query\n",
    "\n",
    "```SQL\n",
    "SELECT title\n",
    "FROM books\n",
    "WHERE author=\"Tennessee Williams\";\n",
    "```\n",
    "\n",
    "returns the title of all books whose author is Tennessee Williams. For each relation (row) in the table, the query engine tests whether the predicate `author=\"Tennessee Williams\"` is true or false. Such use of Boolean and predicate logic is common for several reasons:\n",
    "\n",
    "- Ubiquity: Boolean operations are defined on many data types (strings, integers, etc.)\n",
    "- Performance: Boolean expressions can be efficiently evaluated\n",
    "- Simplicity and adoption: Boolean logic is straightforward and widely used\n",
    "- Theory: logic and set theory are part of the foundations of mathematics\n",
    "\n",
    "However, the complexity that can be expressed with a few Boolean operators is limited. Yet, in the real world, data is often ambiguous, inconsistent or incomplete. Applications which need to deal with uncertainty and inaccuracy include, for example:\n",
    "\n",
    "- Genomic analysis: compare genomic sequences\n",
    "- Web search: rank documents on the WWW based on their relevance to a text query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal Definition\n",
    "\n",
    "The ***k*-nearest neighbor (*k*-NN) search problem** can be defined as follows: given a set of points $X$ in a space $S$ and a query point $\\mathbf{q} \\in S$, find the *k* nearest points in $X$ to $q$.\n",
    "\n",
    "$S$ is usually a metric space, i.e. a set with a metric. Sometimes, however, a similarity measure is used instead of the metric.\n",
    "\n",
    "Imagine a database of book titles. The book titles are strings, or in other words, sequences of symbols taken from an alphabet. A title like \"Les MisÃ©rables\" is an element in the set of all strings $\\Sigma^*$ over the alphabet $\\Sigma$. If we define a string metric $d$, then $S = (d, \\Sigma^*)$ and $X$ is the set of book titles in the database.\n",
    "\n",
    "As with the book database, a metric can sometimes be defined directly on the data. Types of data for which metrics are readily available include strings, real numbers, and georgraphic coordinates. In other cases, data is transformed into a suitable metric space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representations and Metrics\n",
    "\n",
    "#### Sequences\n",
    "\n",
    "Two common metrics for sequences such as strings are the Hamming distance and the Levenshtein distance.\n",
    "\n",
    "The ***Hamming distance*** is a metric on the set of strings with length $n$. It is the number of positions between two strings at which their corresponding symbols are different.\n",
    "\n",
    "For example, the Hamming distance between \"<span style=\"color:cyan\">h</span>ouse\" and \"<span style=\"color:red\">m</span>ouse\" is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hamming_distance(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    return sum(xi != yi for xi, yi in zip(x, y))\n",
    "\n",
    "hamming_distance(\"house\", \"mouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***Levenshtein distance*** is a metric on the set of all strings. It is the minimum number of single-character edits required to change one string into the other. Unlike the Hamming distance, it is therefore defined for strings of any length and is able to take account of insertions and deletions.\n",
    "\n",
    "For example, the Levenshtein distance between \"kitten\" and \"sitting\" is 3. The <span style=\"color:cyan\">k</span> and <span style=\"color:cyan\">e</span> need to be substituted by an <span style=\"color:red\">s</span> and <span style=\"color:red\">i</span>, and a <span style=\"color:red\">g</span> needs to be inserted at the end:\n",
    "\n",
    "<span style=\"color:cyan\">k</span>itt<span style=\"color:cyan\">e</span>n<span style=\"color:cyan\">_</span><br><span style=\"color:red\">s</span>itt<span style=\"color:red\">i</span>n<span style=\"color:red\">g</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequences are defined by the order of their elements. However, for data such as natural language, units such as words matter more than the index of characters in a sequence of text.\n",
    "\n",
    "One representation for text is the ***bag-of-words model***. It represents text as the bag ([multiset](https://en.wikipedia.org/wiki/Multiset)) of its words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 2,\n",
       "         'author': 1,\n",
       "         'of': 1,\n",
       "         'book': 1,\n",
       "         'is': 1,\n",
       "         'Tennessee': 1,\n",
       "         'Williams': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoW = lambda s: Counter(s.split())\n",
    "BoW(\"the author of the book is Tennessee Williams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words model is a natural vector representation: each word is a dimension. The distance betweeen two elements in the set can be thus defined by a metric such as the cosine distance.\n",
    "\n",
    "A drawback of the bag-of-words model is that it represents only the frequency of words and does not capture their order in the sequence.\n",
    "\n",
    "An alternative representation for sequences such as text is the ***n-gram*** model. An n-gram is a contiguous sub-sequence of *n* items from a given sequence. The unit of an n-gram depends on the elements of the given sequence. For text, it can be words or characters, while for a DNA sequence it might be base pairs. For example, the string \"language\" can be split into 5 character 4-grams:\n",
    "\n",
    "lang, angu, ngua, guag, uage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John likes', 'likes to', 'to watch', 'watch movies']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_ngrams(s: str, n=2):\n",
    "    \"\"\"Splits a string into n-grams of n words\"\"\"\n",
    "    words = s.split()\n",
    "    return [\" \".join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "\n",
    "word_ngrams(\"John likes to watch movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['App', 'ppl', 'ple', 'le ', 'e I', ' In', 'Inc', 'nc.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def char_ngrams(s: str, n=3):\n",
    "    \"\"\"Splits a string into n-grams of n characters\"\"\"\n",
    "    return [s[i:i+n] for i in range(len(s)-n+1)]\n",
    "\n",
    "char_ngrams(\"Apple Inc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between the sets of n-grams of two sequences can be measured using a metric such as the Jaccard distance. The Jaccard distance is a metric on the set of all finite sets. For two sets $A$ and $B$, the Jaccard similarity coefficient (also known as the Jaccard index) is given by the size of their intersection divided by the size of their union:\n",
    "\n",
    "$$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "The Jaccard distance is then obtained by substracting the similarity coefficient from $1$:\n",
    "\n",
    "$$d_{Jaccard}(A, B) = 1 - J(A, B)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard(a: set, b: set):\n",
    "    \"\"\"Jaccard distance between two sets a and b\"\"\"\n",
    "    coeff = len(a & b) / len(a | b)\n",
    "    return 1 - coeff\n",
    "\n",
    "# character 3-grams of two DNA sequences\n",
    "a = (\"ATC\", \"TCG\", \"CGA\", \"GAT\")\n",
    "b = (\"CGA\", \"GAT\", \"ATT\", \"TTG\", \"TGA\")\n",
    "jaccard(set(a), set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard(set(a), set(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, a set of n-gram sequences can be vectorized so that the distance between two sequences is measurable using a metric such as the cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TTG</th>\n",
       "      <th>TGA</th>\n",
       "      <th>CGA</th>\n",
       "      <th>TCG</th>\n",
       "      <th>ATT</th>\n",
       "      <th>GAT</th>\n",
       "      <th>ATC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ATCGAT</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGATTGA</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         TTG  TGA  CGA  TCG  ATT  GAT  ATC\n",
       "ATCGAT     0    0    1    1    0    1    1\n",
       "CGATTGA    1    1    1    0    1    1    0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set of n-gram sequences\n",
    "s = set([a, b])\n",
    "# Find the set of all n-grams\n",
    "dims = set.union(*[set(element) for element in s])\n",
    "# Create a feature matrix\n",
    "matrix = np.zeros((len(s), len(dims)))\n",
    "# Populate the matrix\n",
    "for e_idx, element in enumerate(s):\n",
    "    features = Counter(element)\n",
    "    for d_idx, dim in enumerate(dims):\n",
    "        matrix[e_idx][d_idx] = features.get(dim, 0)\n",
    "\n",
    "df = pd.DataFrame(matrix, index=[\"ATCGAT\", \"CGATTGA\"], columns=dims, dtype=int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5527864045000421"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(df.loc[\"ATCGAT\"], df.loc[\"CGATTGA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Word and sentence embeddings*** are two other representations of text. The idea is to represent words or sentences as low-dimensional vectors using a language model such as a neural network. The language model is typically designed to encode the meaning of the word or sentence. Words or sentences with similar meaning or semantic content will thus be close (e.g. small cosine distance) in the vector space.\n",
    "\n",
    "#### Vectors\n",
    "\n",
    "An important subset of metric spaces are normed vector spaces. A vector space is a set of vectors in which two operations are defined: addition and scalar multiplication. Normed vector spaces are vector spaces with a specified norm, i.e. a function which determines the length of all vectors in the vector space. A norm induces a metric, so every normed vector space is a metric space.\n",
    "\n",
    "The $p$-norm or $L^p$ norm of a vector $x$ is defined by\n",
    "\n",
    "$${\\lVert x \\rVert}_p = \\left(\\sum_{i=1}^n |x_i|^p\\right)^{\\frac{1}{p}}$$\n",
    "\n",
    "for a real number $p â‰¥ 1$. For a given $p$-norm, the corresponding distance (also known as Minkowski distance) between two points $x$ and $y$ is:\n",
    "\n",
    "$$d(x,y) = {\\lVert x-y \\rVert}_p$$\n",
    "\n",
    "Based on $p$, we can distinguish between three metrics:\n",
    "- $p=1$:    ***Manhattan distance*** / ***taxicab metric***\n",
    "\n",
    "$$d_{Manhattan}(x, y) = \\sum_{i=1}^n|x_i-y_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def manhattan(x: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"Manhattan distance between two points x and y\"\"\"\n",
    "    return np.sum(np.abs(x - y))\n",
    "\n",
    "x = np.array([0, 0])\n",
    "y = np.array([2, 1])\n",
    "manhattan(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Manhattan distance can be used when the dimensions of the vector space are not comparable. It is faster to compute than the Euclidean distance and may therefore be preferred in very high dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p=2$: ***Euclidean distance***\n",
    "\n",
    "$$d_{Euclidean}(x, y) = \\left(\\sum_{i=1}^n (x_i-y_i)^2\\right)^{\\frac{1}{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.23606797749979"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def euclidean(x: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"Euclidean distance between two points x and y\"\"\"\n",
    "    return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "euclidean(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, metrics are used only for relative distance comparisons. In such cases, computing the absolute distance is not necessary. For the Euclidean distance, we can therefore omit the square root operation. \n",
    "\n",
    "The Euclidean distance is among the most widely used metrics. However, as the difference between the two points is squared, an outlier coordinate can skew the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p=\\infty$: ***Chebychev distance***\n",
    "\n",
    "The $\\infty$-norm is the limit of the $p$-norm for $p \\to \\infty$:\n",
    "\n",
    "$$\\|x\\|_\\infty = \\lim_{p\\to\\infty} \\|x\\|_p = \\sup_i |x_i|$$\n",
    "\n",
    "In a finite-dimensional vector space, the corresponding distance function is the maximum of the absolute difference between $x$ and $y$:\n",
    "\n",
    "$$d_{Chebychev}(x, y) = \\max_i|x_i -y_i|$$\n",
    "\n",
    "See proof [here](https://proofwiki.org/wiki/Chebyshev_Distance_is_Limit_of_P-Product_Metric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chebyshev(x: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"Chebyshev distance between two points x and y\"\"\"\n",
    "    return np.max(np.abs(x - y))\n",
    "\n",
    "chebyshev(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chebyshev distance is useful when we are interested in the largest difference along any single dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Inner product spaces*** are normed vector spaces with an inner product. An ***inner product*** $\\langle \\cdot, \\cdot \\rangle$ is an operation on two vectors that satisifes the *bilinearity*, *symmetry*, and *positive-definiteness* properties. For example, the real vector space $\\mathbb{R}^n$ with $L^2$ norm, also known as Euclidean space, has an inner product that is known as the dot product. For two vectors $x$ and $y$:\n",
    "\n",
    "$$\\langle x, y \\rangle = \\sum_i^n x_i y_i = x \\cdot y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([-1, 3])\n",
    "y = np.array([.5, 2])\n",
    "\n",
    "np.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product indicates the angle and magnitude of two vectors. Two vectors with a small angle can thus have a smaller dot product than two vectors with a larger angle but higher magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x, x) > np.dot(x**2, y**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By dividing the vectors by their norms, i.e. converting them into unit vectors, the effect of magnitude can be eliminated. The result of this operation is known as the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x / np.linalg.norm(x)\n",
    "y = x / np.linalg.norm(x)\n",
    "np.dot(x, x) > np.dot(x**2, y**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***Cosine similarity*** is a measure of the similarity of two vectors $x$ and $y$, and is defined as the cosine of the angle $\\theta$ between them:\n",
    "\n",
    "$$S_C(x, y) := \\cos(\\theta) = \\frac{x \\cdot y}{\\|x\\| \\|y\\|} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8436614877321075"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine(x: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"Cosine similarity between two points x and y\"\"\"\n",
    "    assert np.any(x) == True and np.any(y) == True\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "x = np.array([-1, 3])\n",
    "y = np.array([.5, 2])\n",
    "cosine(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product and cosine similarity are not metrics, but are often used in information retrieval when text is represented as term/n-gram frequency vectors or as word/sentence embeddings.\n",
    "\n",
    "Proportional vectors have a cosine similarity of $1$, orthogonal vectors have a similarity of $0$, and opposite vectors have a similarity of $-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthogonal = np.array([3, 1])\n",
    "cosine(x, orthogonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9999999999999998"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opposite = np.array([1, -3])\n",
    "cosine(x, opposite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures and Algorithms\n",
    "\n",
    "A brute force solution to the *k*-NN search problem is to compute the distance between the query point and each point in dataset $X$, and loop *k* times through the results to return the *k* points with the smallest distance. The downside of this approach is that it has a high computational complexity. Let's imagine, for example, that $S$ is a vector space $\\R^d$ with $d$ dimensions and that the Euclidean distance is used as the metric. Each distance computation has a time complexity of $O(d)$. Computing the distances for $n$ points in $X$ therefore requires $O(nd)$ runtime. At last, selecting the *k* smallest distances requires $O(nk)$ runtime. The time complexity of this brute force algorithm is therefore $O(nd + nk)$. If we have a high number of dimensions, a large number of samples, or need to evaluate multiple queries, this approach is unworkable.\n",
    "\n",
    "A more refined approach is to build an index of the spatial structure of the dataset $X$. Data structures for partitioning space can depend on the metric of the space. \n",
    "\n",
    "General metric spaces:\n",
    "- **vp-tree** (vantage point tree): a tree that partitions space into smaller and smaller circles. The root of the tree is an element of $S$ that serves as the \"vantage point\". Elements that are within a certain distance (radius) of this point are on one side of the node, while elements outside of the radius are on the other side of the node. Partitioning space with this method yields a tree of increasingly smaller circles.\n",
    "- **BK-tree** (Burkhard tree): a tree designed specifically for discrete metrics (e.g. Levenshtein distance; Manhattan and Chebyshev distance on a space of natural numbers). Each node represents an element of $S$, and links between nodes indicate the distance between them.\n",
    "\n",
    "Euclidean spaces:\n",
    "- ***k*-d tree** (*k*-dimensional tree): a type of binary search tree. Every node in the tree is a point in *k*-dimensional space. The two childs of every node are points on the right and left side of a hyperplane along a certain dimension.\n",
    "- **r-tree**: a tree where groups of nearby points are represented by their minimum bounding rectangle (hence the name r-tree). Each node represents a rectangle and links to its sub-rectangles. The r-tree can therefore be thought of a tree of increasingly smaller rectangles.\n",
    "\n",
    "Levenshtein spaces:\n",
    "- **trie** (also know as **prefix tree**): a search tree where each node is a partial or complete sequence and links between nodes represent individual elements.\n",
    "\n",
    "Depth-first search (DFS) of these data structures yields exact results. DFS of exact space indexes works well on low dimensional spaces, but is expensive on high-dimensional ones. For high dimensions, this method is not much more efficient than the brute force approach.\n",
    "\n",
    "<!-- See \"A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces\" -->\n",
    "\n",
    "An alternative approach for high-dimensional spaces is to relax the constraints around the problem. Instead of searching for the exact nearest neighbors, only the approximate nearest neighbors are searched for. The idea behind **approximate nearest neighbors (ANN) search** is to leverage quantization, i.e. reduce the cardinality of the representation, to improve search efficiency. Essentially, the dataset is compressed with loss before being indexed. ANN search therefore makes a tradeoff between search quality and efficiency.\n",
    "\n",
    "One technique to perform quantization is **locality sensitive hashing** (LSH). A locality senstive hash function maps points from $S$ into a lower dimensional representation that can be efficiently indexed and queried.\n",
    "\n",
    "In Euclidean space, another technique is **product quantization**. It partitions a high dimensional space into a Cartesian product of low dimensional subspaces and quantizes each subspace separately (JÃ©gou, Douze, & Schmid, 2011). The distance between a query vector and a PQ-encoded vector can be estimated with low runtime cost (this is known as asymmetric distance computation). Based on product quantization, an **inverted index** of the encoded dataset can be built to efficiently search for nearest neighbors. The inverted index proposed by JÃ©gou, Douze, & Schmid (2011) is similar to the inverted file system of Sivic and Zisserman (2003).\n",
    "\n",
    "The **Hierarchical Navigable Small World** graph (HNSW) is a proximity graph, i.e. its vertices are linked based on their proximity in space (Malkov & Yashunin, 2016). HNSW is inspired by two data structures: *skip lists* and *navigable small world* graphs (NSW). Skip lists consist of layered linked list that allow for fast search. NSW models are graphs with (poly/)logarithmic search complexity that use a greedy routing algorithm. HNSW consists of layered NSW graphs, with a hierachical multi-structure similar to the skip list.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"hnsw.png\" style=\"width:calc(1em * 20); margin:20px; background-color: white;\" alt=\"Hierarchical Navigable Small World graph\"/>\n",
    "    <div style=\"font-size: 0.85em;\">Hierarchical Navigable Small World graph (Malkov & Yashunin, 2016)</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNS in Practice: Wikipedia Search\n",
    "\n",
    "Let's look at how NNS search can be implemented in practice. In this notebook, we'll implement a Wikipedia search engine. For simplicity, we'll index only the names of the writers with an article on the English Wikipedia. The problem to be solved can thus be stated as follows: given the names of all writers on Wikipedia and a string query, find the *k* names that are nearest to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import\n",
    "\n",
    "The data is imported from Wikidata by querying for items whose `occupation` property (P106) is `writer` (Q36180) and who are part of the English Wikipedia. \n",
    "\n",
    "The SPARQL query is optimized to use the label service. See [Wikidata:SPARQL query service/query optimization](https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/query_optimization#Label_service)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://query.wikidata.org/sparql\"\n",
    "query = \"\"\"\n",
    "SELECT ?itemLabel ?sitelink\n",
    "WHERE {\n",
    "  {\n",
    "    SELECT ?item ?sitelink WHERE {\n",
    "      ?item wdt:P106 wd:Q36180.\n",
    "      ?sitelink schema:about ?item;\n",
    "      schema:isPartOf <https://en.wikipedia.org/>.\n",
    "    }\n",
    "  }\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {\"format\": \"json\", \"query\": query})\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response is then extracted into a Pandas `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George Orwell</td>\n",
       "      <td>https://en.wikipedia.org/wiki/George_Orwell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gautama Buddha</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Gautama_Buddha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hafez</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Hafez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Francis Coventry</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Francis_Coventry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sir John Barrow, 1st Baronet</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Sir_John_Barrow,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>Bridget Minamore</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bridget_Minamore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>Hasibe Ã‡erko</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Hasibe_%C3%87erko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100836</th>\n",
       "      <td>Marshall Thornton</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Marshall_Thornton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100837</th>\n",
       "      <td>Mahmood Ashraf Usmani</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mahmood_Ashraf_U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100838</th>\n",
       "      <td>Nicholas Lamar Soutter</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Nicholas_Lamar_S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100839 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              author  \\\n",
       "0                      George Orwell   \n",
       "1                     Gautama Buddha   \n",
       "2                              Hafez   \n",
       "3                   Francis Coventry   \n",
       "4       Sir John Barrow, 1st Baronet   \n",
       "...                              ...   \n",
       "100834              Bridget Minamore   \n",
       "100835                  Hasibe Ã‡erko   \n",
       "100836             Marshall Thornton   \n",
       "100837         Mahmood Ashraf Usmani   \n",
       "100838        Nicholas Lamar Soutter   \n",
       "\n",
       "                                                     link  \n",
       "0             https://en.wikipedia.org/wiki/George_Orwell  \n",
       "1            https://en.wikipedia.org/wiki/Gautama_Buddha  \n",
       "2                     https://en.wikipedia.org/wiki/Hafez  \n",
       "3          https://en.wikipedia.org/wiki/Francis_Coventry  \n",
       "4       https://en.wikipedia.org/wiki/Sir_John_Barrow,...  \n",
       "...                                                   ...  \n",
       "100834     https://en.wikipedia.org/wiki/Bridget_Minamore  \n",
       "100835    https://en.wikipedia.org/wiki/Hasibe_%C3%87erko  \n",
       "100836    https://en.wikipedia.org/wiki/Marshall_Thornton  \n",
       "100837  https://en.wikipedia.org/wiki/Mahmood_Ashraf_U...  \n",
       "100838  https://en.wikipedia.org/wiki/Nicholas_Lamar_S...  \n",
       "\n",
       "[100839 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = r.json()\n",
    "links = [result[\"sitelink\"][\"value\"] for result in data[\"results\"][\"bindings\"]]\n",
    "authors = [result[\"itemLabel\"][\"value\"] for result in data[\"results\"][\"bindings\"]]\n",
    "assert len(links) == len(authors)\n",
    "df = pd.DataFrame({\"author\": authors, \"link\": links})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll run the following queries to test our index. Note that only \"Victor Hugo\" has an exact match. \"DostoÃ¯evski\" is the French spelling of \"Dostoevsky\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"Feynman\", \"Victor Hugo\", \"DostoÃ¯evski\", \"de Cervantes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *n*-grams and Vectorization\n",
    "\n",
    "Let's represent the names of the authors as a count matrix of their 3-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vic', 'ict', 'cto', 'tor', 'or ', 'r H', ' Hu', 'Hug', 'ugo']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the character 3-grams sequence of Victor Hugo\n",
    "char_ngrams(\"Victor Hugo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the matrix of 3-gram counts using scikit-learn's `CountVectorizer` together with the `char_grams` function defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100839, 27864)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer=char_ngrams, lowercase=False)\n",
    "count_matrix = vectorizer.fit_transform(list(df[\"author\"]))\n",
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting vector space has 27864 dimensions.\n",
    "\n",
    "Next, we transform the list of queries using the learned vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 27864)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_matrix = vectorizer.transform(queries)\n",
    "query_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute Force Approach\n",
    "\n",
    "The simplest approach is to do a brute force search, i.e. calculate the distance between the query and all the points in the dataset, and return the *k* points with the smallest distance.\n",
    "\n",
    "scikit-learn's `NearestNeighbors` is a practical interface to some nearest neighbors algorithms. Since we are indexing *n*-gram vectors, we'll use the cosine distance instead of a formal metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: DostoÃ¯evski\n",
      "                             Fyodor Dostoyevsky\n",
      "https://en.wikipedia.org/wiki/Fyodor_Dostoevsky\n",
      "Cosine similarity: 0.5275\n",
      "                             Andrey Dostoyevsky\n",
      "https://en.wikipedia.org/wiki/Andrey_Dostoevsky\n",
      "Cosine similarity: 0.5275\n",
      "                              Lyubov Dostoevskaya\n",
      "https://en.wikipedia.org/wiki/Lyubov_Dostoevskaya\n",
      "Cosine similarity: 0.5417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "K = 3\n",
    "kNN = NearestNeighbors(n_neighbors=K, algorithm=\"brute\", metric=\"cosine\")\n",
    "kNN.fit(count_matrix)\n",
    "\n",
    "# 3 nearest neighbors of the query `DostoÃ¯evski`\n",
    "print(\"Query: DostoÃ¯evski\")\n",
    "similarities, neighbors = kNN.kneighbors(query_matrix[2])\n",
    "for similarity, neighbor in zip(similarities[0], neighbors[0]):\n",
    "    print(df.iloc[neighbor].to_string(index=False))\n",
    "    print(f\"Cosine similarity: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Feynman\n",
      "                              Richard Feynman\n",
      "https://en.wikipedia.org/wiki/Richard_Feynman\n",
      "Cosine similarity: 0.3798\n",
      "Query: Victor Hugo\n",
      "                              Victor Hugo\n",
      "https://en.wikipedia.org/wiki/Victor_Hugo\n",
      "Cosine similarity: 0.0000\n",
      "Query: DostoÃ¯evski\n",
      "                             Fyodor Dostoyevsky\n",
      "https://en.wikipedia.org/wiki/Fyodor_Dostoevsky\n",
      "Cosine similarity: 0.5275\n",
      "Query: de Cervantes\n",
      "                              Miguel de Cervantes\n",
      "https://en.wikipedia.org/wiki/Miguel_de_Cervantes\n",
      "Cosine similarity: 0.2330\n"
     ]
    }
   ],
   "source": [
    "# Nearest neighbor (k=1) of each query\n",
    "K = 1\n",
    "similarities, neighbors = kNN.kneighbors(query_matrix, n_neighbors=K)\n",
    "for query, neighbor, similarity in zip(queries, neighbors, similarities):\n",
    "    print(f\"Query: {query}\")\n",
    "    print(df.iloc[neighbor.item()].to_string(index=False))\n",
    "    print(f\"Cosine similarity: {similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.1 ms Â± 1.64 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit kNN.kneighbors(query_matrix, n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HNSW Approach\n",
    "\n",
    "Recent research on ANN algorithms and machine learning embeddings has led to the creation of a number of ANN libraries. Examples of such libraries are *Facebook AI Similarity Search* (FAISS) and *Non-Metric Space Library* (NMSLIB). For an overview and benchmarks of these libraries, see [ANN Benchmarks](https://github.com/erikbern/ann-benchmarks).\n",
    "\n",
    "Unlike other libraries which are limited to dense spaces, NMSLIB can index sparse spaces. We use a HNSW graph and use the cosine similarity as the distance function. The parameters `M` and `efConstruction` determine the quality of the constructed graph and therefore impact the accuracy and recall of search. The [NMSLIB documentation](https://github.com/nmslib/nmslib/blob/master/manual/methods.md) provided a brief overview of these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HNSW construction graph parameters\n",
    "M = 30\n",
    "efConstruction = 200\n",
    "\n",
    "index = nmslib.init(\n",
    "    method=\"hnsw\",\n",
    "    space=\"cosinesimil_sparse\",\n",
    "    data_type=nmslib.DataType.SPARSE_VECTOR\n",
    ")\n",
    "index.addDataPointBatch(count_matrix)\n",
    "index.createIndex({\n",
    "    \"M\": M,\n",
    "    \"efConstruction\": efConstruction,\n",
    "    \"indexThreadQty\": 2, \n",
    "    \"post\": 0 # No post-processing of the graph\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the `efSearch` parameter influences recall and search time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.setQueryTimeParams({\"efSearch\": 150})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: DostoÃ¯evski\n",
      "                             Fyodor Dostoyevsky\n",
      "https://en.wikipedia.org/wiki/Fyodor_Dostoevsky\n",
      "Cosine similarity: 0.5275\n",
      "                             Andrey Dostoyevsky\n",
      "https://en.wikipedia.org/wiki/Andrey_Dostoevsky\n",
      "Cosine similarity: 0.5275\n",
      "                              Lyubov Dostoevskaya\n",
      "https://en.wikipedia.org/wiki/Lyubov_Dostoevskaya\n",
      "Cosine similarity: 0.5417\n"
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "result = index.knnQueryBatch(query_matrix[2], k=K)\n",
    "neighbors, similarities = result[0]\n",
    "\n",
    "\n",
    "print(\"Query: DostoÃ¯evski\")\n",
    "for neighbor, similarity in zip(neighbors, similarities):\n",
    "    print(df.iloc[neighbor.item()].to_string(index=False))\n",
    "    print(f\"Cosine similarity: {similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Feynman\n",
      "                              Richard Feynman\n",
      "https://en.wikipedia.org/wiki/Richard_Feynman\n",
      "Cosine similarity: 0.3798\n",
      "Query: Victor Hugo\n",
      "                              Victor Hugo\n",
      "https://en.wikipedia.org/wiki/Victor_Hugo\n",
      "Cosine similarity: 0.0000\n",
      "Query: DostoÃ¯evski\n",
      "                             Fyodor Dostoyevsky\n",
      "https://en.wikipedia.org/wiki/Fyodor_Dostoevsky\n",
      "Cosine similarity: 0.5275\n",
      "Query: de Cervantes\n",
      "                              Miguel de Cervantes\n",
      "https://en.wikipedia.org/wiki/Miguel_de_Cervantes\n",
      "Cosine similarity: 0.2330\n"
     ]
    }
   ],
   "source": [
    "K = 1\n",
    "results = index.knnQueryBatch(query_matrix, k=K)\n",
    "for query, result in zip(queries, results):\n",
    "    nneighbor, similarity = result\n",
    "    print(f\"Query: {query}\")\n",
    "    print(df.iloc[nneighbor.item()].to_string(index=False))\n",
    "    print(f\"Cosine similarity: {similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HNSW graph requires time to be built, but querying it is substantially faster than doing a brute force search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.56 ms Â± 173 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit index.knnQueryBatch(query_matrix, k=K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Principles of Mathematical Analysis, 3rd edition (Rudin, 1976)\n",
    "- A relational model of data for large shared data banks (E. F. Codd, 1970) [PDF](https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf)\n",
    "- Data Structures and Algorithms for Nearest Neighbor Search in General Metric Spaces (P. N. Yianilos, 1993) [PDF](http://algorithmics.lsi.upc.edu/docs/practicas/p311-yianilos.pdf)\n",
    "- Database system concepts, 7th edition (A. Silberschatz, H. F. Korth, S. Sudarshan, 2019)\n",
    "- Relational Algebra (Wikipedia) [Link](https://en.wikipedia.org/wiki/Relational_algebra)\n",
    "- Metric space (Wikipedia) [Link](https://en.wikipedia.org/wiki/Metric_space)\n",
    "- String metric (Wikipedia) [Link](https://en.wikipedia.org/wiki/String_metric)\n",
    "- Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs (Y. A. Malkov, D. A. Yashunin, 2016) [Arxiv](https://arxiv.org/abs/1603.09320)\n",
    "- Billion-scale similarity search with GPUs (J. Johnson, M. Douze, H. JÃ©gou, 2019) [Arxiv](https://arxiv.org/abs/1702.08734) / [Github: FAISS library](https://github.com/facebookresearch/faiss)\n",
    "- ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms (M. AumÃ¼ller, E. Bernhardsson, A. Faithfull, 2019) [Arxiv](https://arxiv.org/abs/1807.05614) / [Github](https://github.com/erikbern/ann-benchmarks)\n",
    "- Engineering Efficient and Effective Non-Metric Space Library (L. Boytsov, B. Naidan, 2013) [PDF](http://boytsov.info/pubs/sisap2013.pdf) / [Github: NMSLIB](https://github.com/nmslib/nmslib)\n",
    "- Product Quantization for Nearest Neighbor Search (H. JÃ©gou, M. Douze, C. Schmid,  2011) [HAL-Inria](https://hal.inria.fr/inria-00514462v2)\n",
    "- Video Google: A Text Retrieval Approach to Object Matching in Videos (J. Sivic, and A. Zisserman, 2003) [PDF](https://www.robots.ox.ac.uk/~vgg/publications/papers/sivic03.pdf)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2bb61274428d8154af2dff0fdb211830dcce3790af96cc102fcdb2021a644ac5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
