{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Overview of Approximate Nearest Neighbors Search\n",
    "\n",
    "## Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from collections import Counter\n",
    "from scipy.spatial import distance\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational algebra and SQL\n",
    "\n",
    "In relational algebra, a **theta join** ($⋈_{\\theta}$) is an operation between two relations (tables) $R$ and $S$ that returns the set of all combinations of tuples (rows) in $R$ and $S$ that satisfy a predicate $\\theta$. \n",
    "\n",
    "$$R ⋈_{\\theta} S$$\n",
    "\n",
    "The predicate $\\theta$ is essentially a comparison between attributes (columns) of the two relations. The operators for comparisons are =, ≠, <, ≤, >, and ≥. The literature on relational algebra highlights two cases of the theta join:\n",
    "- The **equijoin**: a theta join where the operator in the predicate is an equal sign (=).\n",
    "- The **natural join**: an equijoin on all common attributes between $R$ and $S$, preserving only one of each compared attributes. The attributes to be compared are implicit and do not need to be specified.\n",
    "\n",
    "In practice, relational algebra is extended to distinguish between inner and outer joins. Inner joins return only the tuples (rows) that satisfy the matching criteria. Outer joins return the matched tuples, plus all other tuples from one or both tables, filling empty attributes with NULL values. There are three outer join operations:\n",
    "- **Left outer join**: preserves all tuples in the relation on the left side of the operation\n",
    "- **Right outer join**: preserves all tuples in the relation on the right side of the operation\n",
    "- **Full outer join**: preserves all tuples in both relations\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"join-operations.png\" style=\"width:calc(1em * 37); margin:20px;\" alt=\"Join Operations\"/>\n",
    "    <div style=\"font-size: 0.85em;\">Illustration of the inner and outer join operations</div>\n",
    "</div>\n",
    "\n",
    "SQL builds upon relational algebra. The SQL `INNER JOIN` operation corresponds to the theta join, with the `ON` keyword specifying the predicate. For example, given the `books` and `stocks` tables, we can find all books available at the Frescati bookshop by joining both tables on the `isbn` and `location` attributes.  \n",
    "\n",
    "`books` table:\n",
    "\n",
    "|isbn (PK)        |author            |title                   |\n",
    "|-----------------|------------------|------------------------|\n",
    "|978-2-253-06707-8|Fiodor Dostoïevski|Les Frères Karamazov    |\n",
    "|978-0-141-19027-3|Tennessee Williams|A Streetcar Named Desire|\n",
    "\n",
    "`stocks` table:\n",
    "\n",
    "|isbn (PK)        |location (PK)  |quantity|\n",
    "|-----------------|---------------|--------|\n",
    "|978-2-253-06707-8|Frescati       |2       |\n",
    "|978-0-141-19027-3|Östermalmstorg |1       |\n",
    "|978-0-141-19027-3|Frescati       |3       |\n",
    "\n",
    "SQL query:\n",
    "\n",
    "```SQL\n",
    "SELECT books.isbn, books.author, books.title, stocks.quantity\n",
    "FROM books\n",
    "INNER JOIN stocks ON books.isbn=stocks.isbn\n",
    "\tAND stocks.location=\"Frescati\";\n",
    "```\n",
    "\n",
    "Result:\n",
    "\n",
    "|isbn             |author            |title                   |quantity|\n",
    "|-----------------|------------------|------------------------|--------|\n",
    "|978-2-253-06707-8|Fiodor Dostoïevski|Les Frères Karamazov    |2       |\n",
    "|978-0-141-19027-3|Tennessee Williams|A Streetcar Named Desire|3       |\n",
    "\n",
    "Note that since the attribute `isbn` is found in both tables, we could also have performed a natural join (`NATURAL JOIN stocks`) and filtered the result with a `WHERE` clause (`WHERE stocks.location=\"Frescati\"`).\n",
    "\n",
    "Relational algebra and SQL are limited in their application to information that is consistent and standardized. The relational model is based on classic propositional logic (boolean algebra) and does therefore not take account of uncertainty and inaccuracy. Yet data is often ambiguous or inconsistent. One example is natural language: a query in the `books` table for \"Fyodor Dostoevsky\" would not return any results. While the French and English spellings are both valid and identify the same author, their string representations cannot be equated using Boolean algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0b1000110 0b1101001 0b1101111 0b1100100 0b1101111 0b1110010 0b100000 0b1000100 0b1101111 0b1110011 0b1110100 0b1101111 0b11000011 0b10101111 0b1100101 0b1110110 0b1110011 0b1101011 0b1101001'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary = lambda s: \" \".join(map(bin,bytearray(s, \"utf-8\")))\n",
    "binary(\"Fiodor Dostoïevski\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0b1000110 0b1111001 0b1101111 0b1100100 0b1101111 0b1110010 0b100000 0b1000100 0b1101111 0b1110011 0b1110100 0b1101111 0b1100101 0b1110110 0b1110011 0b1101011 0b1111001'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary(\"Fyodor Dostoevsky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Fiodor Dostoïevski\" == \"Fyodor Dostoevsky\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other sources of ambiguity and inconsistency include differing standards and imperfect data collection methods.\n",
    "\n",
    "Instead of using boolean algebra to evaluate two relations, we can use their distance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representations and Metrics\n",
    "\n",
    "Attributes can be represented as elements of sets. Strings, for example, are sequences of symbols taken from an alphabet. The name \"Fyodor Dostoevsky\" is therefore an element in the set of all strings $\\Sigma^*$ over the alphabet $\\Sigma$. Representations can be given by the type of the attribute (strings, real numbers, geographic coordinates) or derived by transforming it (n-grams, word embeddings). Based on the definition of the set, the relationship between its elements can be quantified using a metric. A ***metric*** is a function which give the distance between each pair of elements in the set as a real number. A set with a metric is a ***metric space***.\n",
    "\n",
    "For strings, two common metrics are the Hamming distance and the Levenshtein distance.\n",
    "\n",
    "The ***Hamming distance*** is a metric on the set of strings with length $n$. It is the number of positions between two strings at which their corresponding symbols are different.\n",
    "\n",
    "For example, the Hamming distance between \"F<span style=\"color:cyan\">y</span>odor\" and \"F<span style=\"color:red\">i</span>odor\" is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hamming_distance(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    return sum(xi != yi for xi, yi in zip(x, y))\n",
    "\n",
    "hamming_distance(\"Fyodor\", \"Fiodor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***Levenshtein distance*** is a metric on the set of all strings. It is the minimum number of single-character edits required to change one string into the other. Unlike the Hamming distance, it is therefore defined for strings of any length and is able to take account of insertions and deletions.\n",
    "\n",
    "For example, the Levenshtein distance between \"kitten\" and \"sitting\" is 3. The <span style=\"color:cyan\">k</span> and <span style=\"color:cyan\">e</span> need to be substituted by an <span style=\"color:red\">s</span> and <span style=\"color:red\">i</span>, and a <span style=\"color:red\">g</span> needs to be inserted at the end:\n",
    "\n",
    "<span style=\"color:cyan\">k</span>itt<span style=\"color:cyan\">e</span>n<span style=\"color:cyan\">_</span><br><span style=\"color:red\">s</span>itt<span style=\"color:red\">i</span>n<span style=\"color:red\">g</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequences such as strings are defined by the order of their elements. However, for data such as natural language, units such as words matter more than the index of characters in a sequence of text.\n",
    "\n",
    "One representation for text is the ***bag-of-words model***. It represents text as the bag ([multiset](https://en.wikipedia.org/wiki/Multiset)) of its words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 2,\n",
       "         'author': 1,\n",
       "         'of': 1,\n",
       "         'book': 1,\n",
       "         'is': 1,\n",
       "         'Tennessee': 1,\n",
       "         'Williams': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BoW = lambda s: Counter(s.split())\n",
    "BoW(\"the author of the book is Tennessee Williams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words model is a natural vector representation: each word is a dimension. The distance betweeen two elements in the set can be thus defined by a metric such as the cosine distance.\n",
    "\n",
    "A drawback of the bag-of-words model is that it represents only the frequency of words and does not capture their order in the sequence.\n",
    "\n",
    "An alternative representation for sequences such as text is the ***n-gram*** model. An n-gram is a contiguous sub-sequence of *n* items from a given sequence. The unit of an n-gram depends on the elements of the given sequence. For text, it can be words or characters, while for a DNA sequence it might be base pairs. For example, the string \"language\" can be split into 5 character 4-grams:\n",
    "\n",
    "lang, angu, ngua, guag, uage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John likes', 'likes to', 'to watch', 'watch movies']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_ngrams(s: str, n=2):\n",
    "    \"\"\"Splits a string into n-grams of n words\"\"\"\n",
    "    words = s.split()\n",
    "    return [\" \".join(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "\n",
    "word_ngrams(\"John likes to watch movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['App', 'ppl', 'ple', 'le ', 'e I', ' In', 'Inc', 'nc.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def char_ngrams(s: str, n=3):\n",
    "    \"\"\"Splits a string into n-grams of n characters\"\"\"\n",
    "    return [s[i:i+n] for i in range(len(s)-n+1)]\n",
    "\n",
    "char_ngrams(\"Apple Inc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance between the sets of n-grams of two sequences can be measured using a metric such as the Jaccard distance. The Jaccard distance is a metric on the set of all finite sets. For two sets $A$ and $B$, the Jaccard similarity coefficient (also known as the Jaccard index) is given by the size of their intersection divided by the size of their union:\n",
    "\n",
    "$$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "The Jaccard distance is then obtained by substracting the similarity coefficient from $1$:\n",
    "\n",
    "$$d_{Jaccard}(A, B) = 1 - J(A, B)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def jaccard(a: set, b: set):\n",
    "    \"\"\"Jaccard distance between two sets a and b\"\"\"\n",
    "    coeff = len(a & b) / len(a | b)\n",
    "    return 1 - coeff\n",
    "\n",
    "# character 3-grams of two DNA sequences\n",
    "a = (\"ATC\", \"TCG\", \"CGA\", \"GAT\")\n",
    "b = (\"CGA\", \"GAT\", \"ATT\", \"TTG\", \"TGA\")\n",
    "jaccard(set(a), set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard(set(a), set(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, a set of n-gram sequences can be vectorized so that the distance between two sequences is measurable using a metric such as the cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GAT</th>\n",
       "      <th>TGA</th>\n",
       "      <th>ATT</th>\n",
       "      <th>ATC</th>\n",
       "      <th>CGA</th>\n",
       "      <th>TCG</th>\n",
       "      <th>TTG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ATCGAT</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CGATTGA</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         GAT  TGA  ATT  ATC  CGA  TCG  TTG\n",
       "ATCGAT     1    1    1    0    1    0    1\n",
       "CGATTGA    1    0    0    1    1    1    0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set of n-gram sequences\n",
    "s = set([a, b])\n",
    "# Find the set of all n-grams\n",
    "dims = set.union(*[set(element) for element in s])\n",
    "# Create a feature matrix\n",
    "matrix = np.zeros((len(s), len(dims)))\n",
    "# Populate the matrix\n",
    "for e_idx, element in enumerate(s):\n",
    "    features = Counter(element)\n",
    "    for d_idx, dim in enumerate(dims):\n",
    "        matrix[e_idx][d_idx] = features.get(dim, 0)\n",
    "\n",
    "df = pd.DataFrame(matrix, index=[\"ATCGAT\", \"CGATTGA\"], columns=dims, dtype=int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5527864045000421"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(df.loc[\"ATCGAT\"], df.loc[\"CGATTGA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Word and sentence embeddings*** are two other representations of text. The idea is to represent words or sentences as low-dimensional vectors using a language model such as a neural network. The language model is typically designed to encode the meaning of the word or sentence. Words or sentences with similar meaning or semantic content will thus be close (e.g. small cosine distance) in the vector space.\n",
    "\n",
    "<!-- Locality sensitive hashing\n",
    "\n",
    "Combine different dimensions (e.g. name, location, employee count) -->\n",
    "\n",
    "An important subset of metric spaces are normed vector spaces. A vector space is a set of vectors in which two operations are defined: addition and scalar multiplication. Normed vector spaces are vector spaces with a specified norm, i.e. a function which determines the length of all vectors in the vector space. A norm induces a metric, so every normed vector space is a metric space.\n",
    "\n",
    "The $p$-norm or $L^p$ norm of a vector $x$ is defined by\n",
    "\n",
    "$${\\lVert x \\rVert}_p = \\left(\\sum_{i=1}^n |x_i|^p\\right)^{\\frac{1}{p}}$$\n",
    "\n",
    "for a real number $p ≥ 1$. For a given $p$-norm, the corresponding distance (also known as Minkowski distance) between two points $x$ and $y$ is:\n",
    "\n",
    "$$d(x,y) = {\\lVert x-y \\rVert}_p$$\n",
    "\n",
    "Based on $p$, we can distinguish between three metrics:\n",
    "- $p=1$:    ***Manhattan distance*** / ***taxicab metric***\n",
    "\n",
    "$$d_{Manhattan}(x, y) = \\sum_{i=1}^n|x_i-y_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def manhattan(x: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"Manhattan distance between two points x and y\"\"\"\n",
    "    return np.sum(np.abs(x - y))\n",
    "\n",
    "x = np.array([0, 0])\n",
    "y = np.array([2, 1])\n",
    "manhattan(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Manhattan distance can be used when the dimensions of the vector space are not comparable. It is faster to compute than the Euclidean distance and may therefore be preferred in very high dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p=2$: ***Euclidean distance***\n",
    "\n",
    "$$d_{Euclidean}(x, y) = \\left(\\sum_{i=1}^n (x_i-y_i)^2\\right)^{\\frac{1}{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.23606797749979"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def euclidean(x: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"Euclidean distance between two points x and y\"\"\"\n",
    "    return np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "euclidean(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, metrics are used only for relative distance comparisons. In such cases, computing the absolute distance is not necessary. For the Euclidean distance, we can therefore omit the square root operation. \n",
    "\n",
    "The Euclidean distance is among the most widely used metrics. However, as the difference between the two points is squared, an outlier coordinate can skew the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $p=\\infty$: ***Chebychev distance***\n",
    "\n",
    "The $\\infty$-norm is the limit of the $p$-norm for $p \\to \\infty$:\n",
    "\n",
    "$$\\|x\\|_\\infty = \\lim_{p\\to\\infty} \\|x\\|_p = \\sup_i |x_i|$$\n",
    "\n",
    "In a finite-dimensional vector space, the corresponding distance function is the maximum of the absolute difference between $x$ and $y$:\n",
    "\n",
    "$$d_{Chebychev}(x, y) = \\max_i|x_i -y_i|$$\n",
    "\n",
    "See proof [here](https://proofwiki.org/wiki/Chebyshev_Distance_is_Limit_of_P-Product_Metric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chebyshev(x: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"Chebyshev distance between two points x and y\"\"\"\n",
    "    return np.max(np.abs(x - y))\n",
    "\n",
    "chebyshev(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chebyshev distance is useful when we are interested in the largest difference along any single dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Inner product spaces*** are normed vector spaces with an inner product. An ***inner product*** $\\langle \\cdot, \\cdot \\rangle$ is an operation on two vectors that satisifes the *bilinearity*, *symmetry*, and *positive-definiteness* properties. For example, the real vector space $\\R^n$ with $L^2$ norm, also known as Euclidean space, has an inner product that is known as the dot product. For two vectors $x$ and $y$:\n",
    "\n",
    "$$\\langle x, y \\rangle = \\sum_i^n x_i y_i = x \\cdot y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([-1, 3])\n",
    "y = np.array([.5, 2])\n",
    "\n",
    "np.dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product indicates the angle and magnitude of two vectors. Two vectors with a small angle can thus have a smaller dot product than two vectors with a larger angle but higher magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x, x) > np.dot(x**2, y**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By dividing the vectors by their norms, i.e. converting them into unit vectors, the effect of magnitude can be eliminated. The result of this operation is known as the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x / np.linalg.norm(x)\n",
    "y = x / np.linalg.norm(x)\n",
    "np.dot(x, x) > np.dot(x**2, y**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***Cosine similarity*** is a measure of the similarity of two vectors $x$ and $y$, and is defined as the cosine of the angle $\\theta$ between them:\n",
    "\n",
    "$$S_C(x, y) \\coloneqq \\cos(\\theta) = \\frac{x \\cdot y}{\\|x\\| \\|y\\|} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8436614877321075"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine(x: np.ndarray, y: np.ndarray):\n",
    "    \"\"\"Cosine similarity between two points x and y\"\"\"\n",
    "    assert np.any(x) == True and np.any(y) == True\n",
    "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "x = np.array([-1, 3])\n",
    "y = np.array([.5, 2])\n",
    "cosine(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product and cosine similarity are not metrics, but are often used in information retrieval when text is represented as term/n-gram frequency vectors or as word/sentence embeddings.\n",
    "\n",
    "Proportional vectors have a cosine similarity of $1$, orthogonal vectors have a similarity of $0$, and opposite vectors have a similarity of $-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orthogonal = np.array([3, 1])\n",
    "cosine(x, orthogonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9999999999999998"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opposite = np.array([1, -3])\n",
    "cosine(x, opposite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To join two sets based on a metric or similarity measure, we need to find the pairs of elements between both sets that are near to each other. This problem is essentially a form of the k-nearest neighbor search problem. The next section reviews data structures and algorithms to solve this problem efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures and Algorithms\n",
    "\n",
    "The ***k*-nearest neighbor (*k*-NN) search problem** consists in finding in a dataset $X \\subset S$ the *k* nearest points to a query point $\\mathbf{q} \\in S$.\n",
    "\n",
    "A brute force approach would be to compute the distance between the query point and each point in $X$, and loop *k* times through the results to return the *k* points with the smallest distance. The downside of this approach is that it has a high computational complexity. Let's imagine, for example, that $S$ is a vector space $\\R^d$ with $d$ dimensions and that the Euclidean distance is used as the metric. Each distance computation has a time complexity of $O(d)$. Computing the distances for $n$ points in $X$ therefore requires $O(nd)$ runtime. At last, selecting the *k* smallest distances requires $O(nk)$ runtime. The time complexity of this brute force algorithm is therefore $O(nd + nk)$. If we have a high number of dimensions, a large number of samples, or need to evaluate multiple queries, this approach is unworkable.\n",
    "\n",
    "A more refined approach is to build an index of the spacial structure of the dataset $X$. Data structures for partitioning space can depend on the metric of the space. \n",
    "\n",
    "General metric spaces:\n",
    "- **vp-tree** (vantage point tree): a tree that partitions space into smaller and smaller circles. The root of the tree is an element of $S$ that serves as the \"vantage point\". Elements that are within a certain distance (radius) of this point are on one side of the node, while elements outside of the radius are on the other side of the node. Partitioning space with this method yields a tree of increasingly smaller circles.\n",
    "- **BK-tree** (Burkhard tree): a tree designed specifically for discrete metrics (e.g. Levenshtein distance; Manhattan and Chebyshev distance on a space of natural numbers). Each node represents an element of $S$, and links between nodes indicate the distance between them.\n",
    "\n",
    "Euclidean spaces:\n",
    "- ***k*-d tree** (*k*-dimensional tree): a type of binary search tree. Every node in the tree is a point in *k*-dimensional space. The two childs of every node are points on the right and left side of a hyperplane along a certain dimension.\n",
    "- **r-tree**: a tree where groups of nearby points are represented by their minimum bounding rectangle (hence the name r-tree). Each node represents a rectangle and links to its sub-rectangles. The r-tree can therefore be thought of a tree of increasingly smaller rectangles.\n",
    "\n",
    "Levenshtein spaces:\n",
    "- **trie** (also know as **prefix tree**): a search tree where each node is a partial or complete sequence and links between nodes represent individual elements.\n",
    "\n",
    "Depth-first search (DFS) of these data structures yields exact results. DFS of exact space indexes works well on low dimensional spaces, but is expensive on high-dimensional ones. For high dimensions, this method is not much more efficient than the brute force approach.\n",
    "\n",
    "<!-- See \"A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces\" -->\n",
    "\n",
    "An alternative approach for high-dimensional spaces is to relax the constraints around the problem. Instead of searching for the exact nearest neighbors, only the approximate nearest neighbors are searched for. The idea behind **approximate nearest neighbor (ANN) search** is to leverage quantization, i.e. reduce the cardinality of the representation, to improve search efficiency. Essentially, the dataset is compressed with loss before being indexed. ANN search therefore makes a tradeoff between search quality and efficiency.\n",
    "\n",
    "One technique to perform quantization is **locality sensitive hashing** (LSH). A locality senstive hash function maps points from $S$ into a lower dimensional representation that can be efficiently indexed and queried.\n",
    "\n",
    "In Euclidean space, another technique is **product quantization**. It partitions a high dimensional space into a Cartesian product of low dimensional subspaces and quantizes each subspace separately (Jégou, Douze, & Schmid, 2011). The distance between a query vector and a PQ-encoded vector can be estimated with low runtime cost (this is known as asymmetric distance computation). Based on product quantization, an **inverted index** of the encoded dataset can be built to efficiently search for nearest neighbors. The inverted index proposed by Jégou, Douze, & Schmid (2011) is similar to the inverted file system of Sivic and Zisserman (2003).\n",
    "\n",
    "The **Hierarchical Navigable Small World** graph (HNSW) is a proximity graph, i.e. its vertices are linked based on their proximity in space (Malkov & Yashunin, 2016). HNSW is inspired by two data structures: *skip lists* and *navigable small world* graphs (NSW). Skip lists consist of layered linked list that allow for fast search. NSW models are graphs with (poly/)logarithmic search complexity that use a greedy routing algorithm. HNSW consists of layered NSW graphs, with a hierachical multi-structure similar to the skip list.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"hnsw.png\" style=\"width:calc(1em * 20); margin:20px; background-color: white;\" alt=\"Hierarchical Navigable Small World graph\"/>\n",
    "    <div style=\"font-size: 0.85em;\">Hierarchical Navigable Small World graph (Malkov & Yashunin, 2016)</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Search in Practice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"Feynman\", \"Victor Hugo\", \"Dostoïevski\", \"Martin Luther King\", \"Cervantes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPARQL query optimized to use the label service. See [Wikidata:SPARQL query service/query optimization](https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/query_optimization#Label_service)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://query.wikidata.org/sparql\"\n",
    "query = \"\"\"\n",
    "SELECT ?itemLabel ?sitelink\n",
    "WHERE {\n",
    "  {\n",
    "    SELECT ?item ?sitelink WHERE {\n",
    "      ?item wdt:P106 wd:Q36180.\n",
    "      ?sitelink schema:about ?item;\n",
    "      schema:isPartOf <https://en.wikipedia.org/>.\n",
    "    }\n",
    "  }\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],en\". }\n",
    "}\n",
    "\"\"\"\n",
    "r = requests.get(url, params = {\"format\": \"json\", \"query\": query})\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>George Orwell</td>\n",
       "      <td>https://en.wikipedia.org/wiki/George_Orwell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gautama Buddha</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Gautama_Buddha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hafez</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Hafez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Francis Coventry</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Francis_Coventry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sir John Barrow, 1st Baronet</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Sir_John_Barrow,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>Bridget Minamore</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bridget_Minamore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>Hasibe Çerko</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Hasibe_%C3%87erko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100836</th>\n",
       "      <td>Marshall Thornton</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Marshall_Thornton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100837</th>\n",
       "      <td>Mahmood Ashraf Usmani</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Mahmood_Ashraf_U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100838</th>\n",
       "      <td>Nicholas Lamar Soutter</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Nicholas_Lamar_S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100839 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              author  \\\n",
       "0                      George Orwell   \n",
       "1                     Gautama Buddha   \n",
       "2                              Hafez   \n",
       "3                   Francis Coventry   \n",
       "4       Sir John Barrow, 1st Baronet   \n",
       "...                              ...   \n",
       "100834              Bridget Minamore   \n",
       "100835                  Hasibe Çerko   \n",
       "100836             Marshall Thornton   \n",
       "100837         Mahmood Ashraf Usmani   \n",
       "100838        Nicholas Lamar Soutter   \n",
       "\n",
       "                                                     link  \n",
       "0             https://en.wikipedia.org/wiki/George_Orwell  \n",
       "1            https://en.wikipedia.org/wiki/Gautama_Buddha  \n",
       "2                     https://en.wikipedia.org/wiki/Hafez  \n",
       "3          https://en.wikipedia.org/wiki/Francis_Coventry  \n",
       "4       https://en.wikipedia.org/wiki/Sir_John_Barrow,...  \n",
       "...                                                   ...  \n",
       "100834     https://en.wikipedia.org/wiki/Bridget_Minamore  \n",
       "100835    https://en.wikipedia.org/wiki/Hasibe_%C3%87erko  \n",
       "100836    https://en.wikipedia.org/wiki/Marshall_Thornton  \n",
       "100837  https://en.wikipedia.org/wiki/Mahmood_Ashraf_U...  \n",
       "100838  https://en.wikipedia.org/wiki/Nicholas_Lamar_S...  \n",
       "\n",
       "[100839 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = r.json()\n",
    "links = [result[\"sitelink\"][\"value\"] for result in data[\"results\"][\"bindings\"]]\n",
    "authors = [result[\"itemLabel\"][\"value\"] for result in data[\"results\"][\"bindings\"]]\n",
    "assert len(links) == len(authors)\n",
    "df = pd.DataFrame({\"author\": authors, \"link\": links})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *n*-grams and Vectorization\n",
    "\n",
    "Let's represent the names of the authors as a count matrix of their 3-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Vic', 'ict', 'cto', 'tor', 'or ', 'r H', ' Hu', 'Hug', 'ugo']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the character 3-grams sequence of Victor Hugo\n",
    "char_ngrams(\"Victor Hugo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the matrix of 3-gram counts using scikit-learn's `CountVectorizer` together with the `char_grams` function defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100839, 27864)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer=char_ngrams, lowercase=False)\n",
    "count_matrix = vectorizer.fit_transform(list(df[\"author\"]))\n",
    "count_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting vector space has 27864 dimensions.\n",
    "\n",
    "Next, we transform the list of queries using the learned vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 27864)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_matrix = vectorizer.transform(queries)\n",
    "query_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute Force Approach\n",
    "\n",
    "The simplest approach is to do a brute force search, i.e. calculate the distance between the query and all the points in the dataset, and return the *k* points with the smallest distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Dostoïevski\n",
      "                             Fyodor Dostoyevsky\n",
      "https://en.wikipedia.org/wiki/Fyodor_Dostoevsky\n",
      "Cosine similarity: 0.5275\n",
      "                             Andrey Dostoyevsky\n",
      "https://en.wikipedia.org/wiki/Andrey_Dostoevsky\n",
      "Cosine similarity: 0.5275\n",
      "                              Lyubov Dostoevskaya\n",
      "https://en.wikipedia.org/wiki/Lyubov_Dostoevskaya\n",
      "Cosine similarity: 0.5417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "K = 3\n",
    "kNN = NearestNeighbors(n_neighbors=K, algorithm=\"brute\", metric=\"cosine\")\n",
    "kNN.fit(count_matrix)\n",
    "\n",
    "# 3 nearest neighbors of the query `Dostoïevski`\n",
    "print(\"Query: Dostoïevski\")\n",
    "distances, neighbors = kNN.kneighbors(query_matrix[2])\n",
    "for distance, neighbor in zip(distances[0], neighbors[0]):\n",
    "    print(df.iloc[neighbor].to_string(index=False))\n",
    "    print(f\"Cosine similarity: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Feynman\n",
      "                              Richard Feynman\n",
      "https://en.wikipedia.org/wiki/Richard_Feynman\n",
      "Cosine similarity: 0.3798\n",
      "Query: Victor Hugo\n",
      "                              Victor Hugo\n",
      "https://en.wikipedia.org/wiki/Victor_Hugo\n",
      "Cosine similarity: 0.0000\n",
      "Query: Dostoïevski\n",
      "                             Fyodor Dostoyevsky\n",
      "https://en.wikipedia.org/wiki/Fyodor_Dostoevsky\n",
      "Cosine similarity: 0.5275\n",
      "Query: Martin Luther King\n",
      "                            Martin Luther King Jr.\n",
      "https://en.wikipedia.org/wiki/Martin_Luther_Kin...\n",
      "Cosine similarity: 0.1056\n",
      "Query: Cervantes\n",
      "                              Annabel Cervantes\n",
      "https://en.wikipedia.org/wiki/Annabel_Cervantes\n",
      "Cosine similarity: 0.3169\n"
     ]
    }
   ],
   "source": [
    "# Nearest neighbor (k=1) of each query\n",
    "K = 1\n",
    "distances, neighbors = kNN.kneighbors(query_matrix, n_neighbors=K)\n",
    "for query, neighbor, distance in zip(queries, neighbors, distances):\n",
    "    print(f\"Query: {query}\")\n",
    "    print(df.iloc[neighbor.item()].to_string(index=False))\n",
    "    print(f\"Cosine similarity: {distance.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Cervantes\n",
      "                              Annabel Cervantes\n",
      "https://en.wikipedia.org/wiki/Annabel_Cervantes\n",
      "Cosine similarity: 0.3169\n",
      "                              Miguel de Cervantes\n",
      "https://en.wikipedia.org/wiki/Miguel_de_Cervantes\n",
      "Cosine similarity: 0.3583\n",
      "                              Lorna Dee Cervantes\n",
      "https://en.wikipedia.org/wiki/Lorna_Dee_Cervantes\n",
      "Cosine similarity: 0.3583\n"
     ]
    }
   ],
   "source": [
    "print(\"Query: Cervantes\")\n",
    "distances, neighbors = kNN.kneighbors(query_matrix[4])\n",
    "for distance, neighbor in zip(distances[0], neighbors[0]):\n",
    "    print(df.iloc[neighbor].to_string(index=False))\n",
    "    print(f\"Cosine similarity: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.9 ms ± 514 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit kNN.kneighbors(query_matrix, n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HNSW Approach\n",
    "\n",
    "Recent research on ANN algorithms and machine learning embeddings has led to the creation of a number of ANN libraries. Examples of such libraries are *Facebook AI Similarity Search* (FAISS) and *Non-Metric Space Library* (NMSLIB). For an overview and benchmarks of these libraries, see [ANN Benchmarks](https://github.com/erikbern/ann-benchmarks).\n",
    "\n",
    "Sparse Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HNSW construction graph parameters\n",
    "M = 30\n",
    "efConstruction = 200\n",
    "\n",
    "index = nmslib.init(\n",
    "    method=\"hnsw\",\n",
    "    space=\"cosinesimil_sparse\",\n",
    "    data_type=nmslib.DataType.SPARSE_VECTOR\n",
    ")\n",
    "index.addDataPointBatch(count_matrix)\n",
    "index.createIndex({\n",
    "    \"M\": M,\n",
    "    \"efConstruction\": efConstruction,\n",
    "    \"indexThreadQty\": 2, \n",
    "    \"post\": 0 # No post-processing of the graph\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.setQueryTimeParams({\"efSearch\": 150})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Dostoïevski\n",
      "                             Fyodor Dostoyevsky\n",
      "https://en.wikipedia.org/wiki/Fyodor_Dostoevsky\n",
      "Distance: 0.47245562076568604\n",
      "                             Andrey Dostoyevsky\n",
      "https://en.wikipedia.org/wiki/Andrey_Dostoevsky\n",
      "Distance: 0.47245562076568604\n",
      "                              Lyubov Dostoevskaya\n",
      "https://en.wikipedia.org/wiki/Lyubov_Dostoevskaya\n",
      "Distance: 0.4583492875099182\n"
     ]
    }
   ],
   "source": [
    "K = 3\n",
    "result = index.knnQueryBatch(query_matrix[2], k=K)\n",
    "neighbors, distances = result[0]\n",
    "\n",
    "\n",
    "print(\"Query: Dostoïevski\")\n",
    "for neighbor, distance in zip(neighbors, distances):\n",
    "    print(df.iloc[neighbor.item()].to_string(index=False))\n",
    "    print(f\"Distance: {1 - distance.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Feynman\n",
      "                              Richard Feynman\n",
      "https://en.wikipedia.org/wiki/Richard_Feynman\n",
      "Distance: 0.6201736330986023\n",
      "Query: Victor Hugo\n",
      "                              Victor Hugo\n",
      "https://en.wikipedia.org/wiki/Victor_Hugo\n",
      "Distance: 1.0\n",
      "Query: Dostoïevski\n",
      "                             Andrey Dostoyevsky\n",
      "https://en.wikipedia.org/wiki/Andrey_Dostoevsky\n",
      "Distance: 0.47245562076568604\n",
      "Query: Martin Luther King\n",
      "                            Martin Luther King Jr.\n",
      "https://en.wikipedia.org/wiki/Martin_Luther_Kin...\n",
      "Distance: 0.8944271802902222\n",
      "Query: Cervantes\n",
      "                              Annabel Cervantes\n",
      "https://en.wikipedia.org/wiki/Annabel_Cervantes\n",
      "Distance: 0.6831300258636475\n"
     ]
    }
   ],
   "source": [
    "K = 1\n",
    "results = index.knnQueryBatch(query_matrix, k=K)\n",
    "for query, result in zip(queries, results):\n",
    "    nneighbor, distance = result\n",
    "    print(f\"Query: {query}\")\n",
    "    print(df.iloc[nneighbor.item()].to_string(index=False))\n",
    "    print(f\"Distance: {1 - distance.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2 ms ± 263 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit index.knnQueryBatch(query_matrix, k=K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Principles of Mathematical Analysis, 3rd edition (Rudin, 1976)\n",
    "- A relational model of data for large shared data banks (E. F. Codd, 1970) [PDF](https://www.seas.upenn.edu/~zives/03f/cis550/codd.pdf)\n",
    "- Data Structures and Algorithms for Nearest Neighbor Search in General Metric Spaces (P. N. Yianilos, 1993) [PDF](http://algorithmics.lsi.upc.edu/docs/practicas/p311-yianilos.pdf)\n",
    "- Database system concepts, 7th edition (A. Silberschatz, H. F. Korth, S. Sudarshan, 2019)\n",
    "- Relational Algebra (Wikipedia) [Link](https://en.wikipedia.org/wiki/Relational_algebra)\n",
    "- Metric space (Wikipedia) [Link](https://en.wikipedia.org/wiki/Metric_space)\n",
    "- String metric (Wikipedia) [Link](https://en.wikipedia.org/wiki/String_metric)\n",
    "- Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs (Y. A. Malkov, D. A. Yashunin, 2016) [Arxiv](https://arxiv.org/abs/1603.09320)\n",
    "- Billion-scale similarity search with GPUs (J. Johnson, M. Douze, H. Jégou, 2019) [Arxiv](https://arxiv.org/abs/1702.08734) / [Github: FAISS library](https://github.com/facebookresearch/faiss)\n",
    "- ANN-Benchmarks: A Benchmarking Tool for Approximate Nearest Neighbor Algorithms (M. Aumüller, E. Bernhardsson, A. Faithfull, 2019) [Arxiv](https://arxiv.org/abs/1807.05614) / [Github](https://github.com/erikbern/ann-benchmarks)\n",
    "- Engineering Efficient and Effective Non-Metric Space Library (L. Boytsov, B. Naidan, 2013) [PDF](http://boytsov.info/pubs/sisap2013.pdf) / [Github: NMSLIB](https://github.com/nmslib/nmslib)\n",
    "- Product Quantization for Nearest Neighbor Search (H. Jégou, M. Douze, C. Schmid,  2011) [PDF](https://hal.inria.fr/inria-00514462v2)\n",
    "- Video Google: A Text Retrieval Approach to Object Matching in Videos (J. Sivic, and A. Zisserman, 2003) [PDF](https://www.robots.ox.ac.uk/~vgg/publications/papers/sivic03.pdf)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2bb61274428d8154af2dff0fdb211830dcce3790af96cc102fcdb2021a644ac5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
